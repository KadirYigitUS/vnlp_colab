<AI-input-vnlp-colab-project_2025-11-14_11.26.zip.xml>
<file_summary>
This section contains a summary of this file.
<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>
<file_format>
The content is organized as follows:
1. This summary section
2. Directory structure
3. File contents
</file_format>
<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
</usage_guidelines>
<notes>
- Some files may have been excluded based on the script's configuration.
- Binary files are not included in this packed representation.
</notes>
</file_summary>
<directory_structure>
./
├── pyproject.toml
├── tests/
│   ├── __init__.py
│   ├── test_dep.py
│   ├── test_ner.py
│   ├── test_pipeline.py
│   ├── test_pos.py
│   └── test_stemmer.py
└── vnlp_colab/
    ├── dep/
    │   ├── dep_colab.py
    │   ├── dep_treestack_utils_colab.py
    │   ├── dep_utils_colab.py
    │   └── __init__.py
    ├── __init__.py
    ├── ner/
    │   ├── __init__.py
    │   ├── ner_colab.py
    │   └── ner_utils_colab.py
    ├── normalizer/
    │   ├── _deasciifier.py
    │   ├── __init__.py
    │   └── normalizer_colab.py
    ├── pipeline_colab.py
    ├── pos/
    │   ├── __init__.py
    │   ├── pos_colab.py
    │   ├── pos_treestack_utils_colab.py
    │   └── pos_utils_colab.py
    ├── sentiment/
    │   ├── __init__.py
    │   ├── sentiment_colab.py
    │   └── sentiment_utils_colab.py
    ├── stemmer/
    │   ├── __init__.py
    │   ├── stemmer_colab.py
    │   ├── stemmer_utils_colab.py
    │   └── _yildiz_analyzer.py
    ├── tokenizer_colab.py
    └── utils_colab.py

9 directories, 32 files
</directory_structure>
<files>
This section contains the contents of the repository's files.

  <file path="./pyproject.toml">
# pyproject.toml
[build-system]
requires = ["setuptools&gt;=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "vnlp_colab"
version = "1.0.0"
authors = [
  { name="VNLP Project Authors" },
]
description = "A Colab-optimized version of the VNLP library for high-performance Turkish NLP."
readme = "README.md"
license = { text = "AGPL-3.0" }
requires-python = "&gt;=3.10"
classifiers = [
    "Programming Language :: Python :: 3",
    "License :: OSI Approved :: GNU Affero General Public License v3 or later (AGPLv3+)",
    "Operating System :: OS Independent",
    "Topic :: Text Processing :: Linguistic",
]

# All required dependencies for the pipeline to run in a fresh environment.
dependencies = [
    "tensorflow&gt;=2.15.0",
    "keras&gt;=3.0.0",
    "numpy&gt;=1.26.0",
    "pandas&gt;=2.0.0",
    "sentencepiece==0.2.1",
    "spylls&gt;=0.1.5",
    "tqdm&gt;=4.62.0",
    "regex&gt;=2023.6.3",
    "requests&gt;=2.28.0",
]

[project.urls]
"Homepage" = "https://github.com/vngrs-ai/vnlp"
"Bug Tracker" = "https://github.com/vngrs-ai/vnlp/issues"
  </file>

  <file path="./vnlp_colab/normalizer/normalizer_colab.py">
# coding=utf-8
#
# Copyright 2025 VNLP Project Authors.
#
# Licensed under the GNU Affero General Public License, Version 3.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.gnu.org/licenses/agpl-3.0.html
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Full-featured, stateful Normalizer for VNLP Colab.

This module provides a comprehensive Normalizer class with lazy-loading for heavy
resources (dictionaries, lexicons) and dependency injection for the StemmerAnalyzer.
It is designed for high-performance, repeated use within a pipeline.
"""
import logging
import re
from typing import List, Optional, Dict
from pathlib import Path

# Try to import optional dependency spylls for typo correction
try:
    from spylls.hunspell import Dictionary
except ImportError:
    Dictionary = None

# Updated imports for package structure
from vnlp_colab.normalizer._deasciifier import Deasciifier
from vnlp_colab.utils_colab import download_resource
from vnlp_colab.stemmer.stemmer_colab import StemmerAnalyzer

# Forward-declare StemmerAnalyzer for type hinting to avoid circular import
class StemmerAnalyzer:
    pass

logger = logging.getLogger(__name__)

# --- Resource URLs ---
LEXICON_URL = "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/vnlp/resources/turkish_known_words_lexicon.txt"
HUNSPELL_AFF_URL = "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/vnlp/resources/tdd-hunspell-tr-1.1.0/tr_TR.aff"
HUNSPELL_DIC_URL = "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/vnlp/resources/tdd-hunspell-tr-1.1.0/tr_TR.dic"


class Normalizer:
    """
    Stateful Normalizer class with lazy-loading and dependency injection.
    """
    _LOWERCASE_MAP = { "İ": "i", "I": "ı", "Ğ": "ğ", "Ü": "ü", "Ö": "ö", "Ş": "ş", "Ç": "ç" }
    _ACCENT_MAP = { 'â':'a', 'ô':'o', 'î':'i', 'ê':'e', 'û':'u', 'Â':'A', 'Ô':'O', 'Î':'İ', 'Ê':'E', 'Û': 'U' }
    _PUNCTUATION_CHARS = ".,!?;:'\""

    def __init__(self, stemmer_analyzer_instance: Optional[StemmerAnalyzer] = None):
        self._words_lexicon: Optional[Dict[str, None]] = None
        self._stemmer_analyzer: Optional[StemmerAnalyzer] = None
        self._dictionary: Optional['Dictionary'] = None
        self._shared_stemmer_analyzer = stemmer_analyzer_instance

    # --- Private Lazy Loaders ---
    def _load_lexicon(self):
        if self._words_lexicon is None:
            logger.info("Lazy-loading Turkish known words lexicon...")
            lexicon_path = download_resource("turkish_known_words_lexicon.txt", LEXICON_URL)
            with open(lexicon_path, 'r', encoding='utf-8') as f:
                self._words_lexicon = dict.fromkeys(line.strip() for line in f)

    def _load_dictionary(self):
        if self._dictionary is None:
            if Dictionary is None:
                logger.warning("`spylls` not installed. Typo correction is disabled. Run 'pip install spylls'.")
                return
            logger.info("Lazy-loading Hunspell dictionary...")
            aff_path = download_resource("tr_TR.aff", HUNSPELL_AFF_URL)
            dic_path = download_resource("tr_TR.dic", HUNSPELL_DIC_URL)
            self._dictionary = Dictionary.from_files(str(dic_path.parent / 'tr_TR'))

    def _load_stemmer_analyzer(self):
        if self._stemmer_analyzer is None:
            logger.info("Lazy-loading StemmerAnalyzer for typo correction...")
            if self._shared_stemmer_analyzer:
                self._stemmer_analyzer = self._shared_stemmer_analyzer
            else:
                # Late import to prevent circular dependency
                from stemmer_colab import get_stemmer_analyzer
                self._stemmer_analyzer = get_stemmer_analyzer()

    # --- Static Methods ---
    @staticmethod
    def lower_case(text: str) -&gt; str:
        if not isinstance(text, str): return ""
        for k, v in Normalizer._LOWERCASE_MAP.items():
            text = text.replace(k, v)
        return text.lower()

    @staticmethod
    def remove_accent_marks(text: str) -&gt; str:
        """Removes accent marks. This version is corrected to ensure î -&gt; i."""
        if not isinstance(text, str): return ""
        return "".join(Normalizer._ACCENT_MAP.get(char, char) for char in text)

    @staticmethod
    def remove_punctuations(text: str) -&gt; str:
        if not isinstance(text, str): return ""
        return ''.join([t for t in text if (t.isalnum() or t == " ")])

    @staticmethod
    def deasciify(tokens: List[str]) -&gt; List[str]:
        if not tokens: return []
        return [Deasciifier(token).convert_to_turkish() for token in tokens]

    # --- Stateful Methods ---
    def correct_typos(self, tokens: List[str]) -&gt; List[str]:
        """
        Note: This function was historically removed from VNLP due to packaging
        issues, as noted in GitHub issue #18. This version re-implements it
        using `spylls` and integrates it with other VNLP components.
        """
        self._load_dictionary()
        self._load_stemmer_analyzer()
        self._load_lexicon()

        if not self._dictionary or not self._stemmer_analyzer or not self._words_lexicon:
            logger.warning("Typo correction dependencies not fully loaded. Skipping correction.")
            return tokens

        corrected_tokens = []
        for token in tokens:
            l_punct, core_word, t_punct = self._strip_punctuation(token)
            if not core_word:
                corrected_tokens.append(token)
                continue

            analysis = self._stemmer_analyzer.candidate_generator.get_analysis_candidates(core_word)
            is_valid_by_stemmer = (analysis and analysis[0] and 'Unknown' not in analysis[0][2])
            is_known = (core_word.isnumeric() or
                        self._dictionary.lookup(core_word) or
                        self._dictionary.lookup(core_word.lower()) or
                        core_word in self._words_lexicon or
                        core_word.lower() in self._words_lexicon or
                        is_valid_by_stemmer)

            if is_known:
                corrected_tokens.append(token)
            else:
                suggestions = list(self._dictionary.suggest(core_word))
                corrected_word = suggestions[0] if suggestions else core_word
                corrected_tokens.append(l_punct + corrected_word + t_punct)
                
        return corrected_tokens

    def _strip_punctuation(self, token: str) -&gt; tuple[str, str, str]:
        """Separates leading/trailing punctuation from a core word."""
        leading_punct = ""
        while token and token[0] in self._PUNCTUATION_CHARS:
            leading_punct += token[0]
            token = token[1:]
        trailing_punct = ""
        while token and token[-1] in self._PUNCTUATION_CHARS:
            trailing_punct = token[-1] + trailing_punct
            token = token[:-1]
        return leading_punct, token, trailing_punct

    def convert_numbers_to_words(self, tokens: List[str], num_dec_digits: int = 6, decimal_separator: str = ',') -&gt; List[str]:
        converted_tokens = []
        for token in tokens:
            # Prepare token for float conversion
            processed_token = token
            if any(char.isnumeric() for char in token):
                if decimal_separator == ',':
                    processed_token = processed_token.replace('.', '').replace(',', '.')
                elif decimal_separator == '.':
                    processed_token = processed_token.replace(',', '')
                else:
                    raise ValueError(f"'{decimal_separator}' is not a valid decimal separator. Use '.' or ','.")
            
            # Attempt conversion
            try:
                num = float(processed_token)
                converted_tokens.extend(self._num_to_words(num, num_dec_digits).split())
            except (ValueError, TypeError):
                converted_tokens.append(token) # Append original token if not a number
        return converted_tokens

    def _int_to_words(self, main_num: int) -&gt; str:
        """Converts an integer to its Turkish word representation, with improvements."""
        if main_num == 0:
            return "sıfır"
        if main_num &lt; 0:
            return f"eksi {self._int_to_words(abs(main_num))}"

        tp = ["", " bin", " milyon", " milyar", " trilyon", " katrilyon"]
        dec = ["", " bir", " iki", " üç", " dört", " beş", " altı", " yedi", " sekiz", " dokuz"]
        ten = ["", " on", " yirmi", " otuz", " kırk", " elli", " altmış", " yetmiş", " seksen", " doksan"]
        
        num_str = str(main_num)
        num_len = len(num_str)
        groups = (num_len + 2) // 3
        num_str = num_str.zfill(groups * 3)
        
        text_parts = []
        for i in range(groups):
            group_val = int(num_str[i*3 : (i+1)*3])
            if group_val == 0:
                continue
            
            h = group_val // 100
            t = (group_val % 100) // 10
            u = group_val % 10
            
            group_text = ""
            if h &gt; 0:
                group_text += " yüz" if h == 1 else f"{dec[h]} yüz"
            group_text += f" {ten[t]}" if t &gt; 0 else ""
            group_text += f" {dec[u]}" if u &gt; 0 else ""
            
            # Handle "bir bin" -&gt; "bin"
            if group_text.strip() == "bir" and i == groups - 2:
                 text_parts.append(tp[groups - 1 - i])
            else:
                 text_parts.append(group_text + tp[groups - 1 - i])

        return " ".join(part.strip() for part in text_parts if part).strip()

    def _num_to_words(self, num: float, num_dec_digits: int) -&gt; str:
        """Converts a float to its Turkish word representation."""
        integer_part = int(num)
        
        if abs(num - integer_part) &lt; (10**-num_dec_digits):
            return self._int_to_words(integer_part)

        str_decimal = f"{num:.{num_dec_digits}f}".split('.')[1].rstrip('0')
        if not str_decimal:
            return self._int_to_words(integer_part)

        decimal_part = int(str_decimal)
        return f"{self._int_to_words(integer_part)} virgül {self._int_to_words(decimal_part)}"
  </file>

  <file path="./vnlp_colab/normalizer/_deasciifier.py">
# -*- coding: utf-8 -*-
import string
class Deasciifier:
      """
      This class provides a function to deasciify a given Turkish text.
      Input text is assumed to be utf-8 encoded and the output text is in Unicode
      so do not forget to encode the result.

      Example usage:

      my_ascii_turkish_txt = "Opusmegi cagristiran catirtilar."
      deasciifier = Deasciifier(my_ascii_turkish_txt.decode("utf-8"))
      my_deasciified_turkish_txt = deasciifier.convert_to_turkish()
      print my_deasciified_turkish_txt.encode("utf-8")
      """

      turkish_pattern_table = {u'c':
                               {"bu aXa": -1, "Cki Xi": -2, "na Xog": 3,
                                "ram Xo": 4, "gol aX": 5, "huyu X": -6,
                                #-------- truncated -------------------#
                                "Xi ": -2561, "Xu": -2562, "eXe": -2563,
                                "aXa": -2564, "nX": -2565, "X": 2566},
                               u'g':
                               {" s iX": 1, " oraX": -2, "loXi ": 3,
                                "itelX": 4, "zilXi": 5, "r oXr": -6,
                                #-------- truncated -------------------#
                                "OX": 750, "lX": -751, "oX": 752,
                                "uX": 753, "aX": 754, "Xi": 756, "X": -757},
                               u'o':
                               {"uz kXr": -1, "ni kXt": 2, "dir gX": 3,
                                "Il dXn": 4, "an Xng": 5, "lum kX": -6,
                                #-------- truncated -------------------#
                                " Xrne": 1612, " yXn": 1613, " Xd": 1614, " dXn": 1615,
                                " bXl": 1616, "Xgre": 1617, " Xnc": 1618, "Xyle": 1619,
                                "Xne": 1620, "Xz": 1621, " gX": 1622, "X": -1623},

                               u's': {"kut Xe": 1, "a nurX": 2, "lIXiyi": -3, "Xleriz": -4, "raXama": 5,
                                      "de aX ": -6, "vard X": 7, "irmeX ": 8, "gin Xo": -9, "k miX ": 10,
                                       #-------- truncated -------------------#
                                      " kiXi": 3203, "iXle": 3204, "uXtu": 3205, "miX ": 3206, " dUX": 3207,
                                      " karXi": 3208, "Xtir": 3209, "Xm": 3210, "IX": 3211, "baX": 3212, "X": -3213},
                               u'u': {"Xctugu": -1, " ay sX": 2, "fXtur ": 3, "Xysuz ": -4,
                                      "Gi rXs": -5, "an Xnv": 6, "Xldeni": -7, "dIn Xn": 8, "i kXcu": 9,
                                      "43 sX ": 10, "Xcunda": -11, "g kulX": 12, " aC kX": -13, "Xrunda": -14,
                                       #-------- truncated -------------------#
                                      "Xre": 2402, "OrX": 2403, "yXk": 2404, "Xze": 2405, "tXrk": 2406,
                                      "gX": 2407, "X": -2408},
                               u'i': {"n kXsan": -1, " nin Xn": -2, "tIyor X": -3, " armanX": -4,
                                      "Xstirab": 5, "aktXgim": 6, "eci Xsi": 7, "er de X": -8, "ere Xsi": 9,
                                      "ne takX": 10, "ratan X": -11, "Uyen Xn": 12, " alanXs": -13, "sI sXr ": -14,
                                       #-------- truncated -------------------#
                                      "asX": 2972, "InX": 2973, "arX": 2974, "X": -2975}}


      turkish_context_size = 10

      turkish_asciify_table = {u'ç': u'c',
                               u'Ç': u'C',
                               u'ğ': u'g',
                               u'Ğ': u'G',
                               u'ö': u'o',
                               u'Ö': u'O',
                               u'ü': u'u',
                               u'Ü': u'U',
                               u'ı': u'i',
                               u'İ': u'I',
                               u'ş': u's',
                               u'Ş': u'S'}

      turkish_downcase_asciify_table = {}
      for ch in string.ascii_uppercase:
            turkish_downcase_asciify_table[ch] = ch.lower()
            turkish_downcase_asciify_table[ch.lower()] = ch.lower()
      turkish_downcase_asciify_table[u'ç'] = u'c'
      turkish_downcase_asciify_table[u'Ç'] = u'c'
      turkish_downcase_asciify_table[u'ğ'] = u'g'
      turkish_downcase_asciify_table[u'Ğ'] = u'g'
      turkish_downcase_asciify_table[u'ö'] = u'o'
      turkish_downcase_asciify_table[u'Ö'] = u'o'
      turkish_downcase_asciify_table[u'ı'] = u'i'
      turkish_downcase_asciify_table[u'İ'] = u'i'
      turkish_downcase_asciify_table[u'ş'] = u's'
      turkish_downcase_asciify_table[u'Ş'] = u's'
      turkish_downcase_asciify_table[u'ü'] = u'u'
      turkish_downcase_asciify_table[u'Ü'] = u'u'

      turkish_upcase_accents_table = {}
      for ch in string.ascii_uppercase:
            turkish_upcase_accents_table[ch] = ch.lower()
            turkish_upcase_accents_table[ch.lower()] = ch.lower()
      turkish_upcase_accents_table[u'ç'] = u'C'
      turkish_upcase_accents_table[u'Ç'] = u'C'
      turkish_upcase_accents_table[u'ğ'] = u'G'
      turkish_upcase_accents_table[u'Ğ'] = u'G'
      turkish_upcase_accents_table[u'ö'] = u'O'
      turkish_upcase_accents_table[u'Ö'] = u'O'
      turkish_upcase_accents_table[u'ı'] = u'I'
      turkish_upcase_accents_table[u'İ'] = u'i'
      turkish_upcase_accents_table[u'ş'] = u'S'
      turkish_upcase_accents_table[u'Ş'] = u'S'
      turkish_upcase_accents_table[u'ü'] = u'U'
      turkish_upcase_accents_table[u'Ü'] = u'U'



      def __init__(self, ascii_string):
            self.ascii_string = ascii_string
            self.turkish_string = ascii_string

      def print_turkish_string(self):
            print(self.turkish_string)

      def set_char_at(self, mystr, pos, c):
            return mystr[0:pos] + c + mystr[pos+1:]

      def convert_to_turkish(self):
            """Convert a string with ASCII-only letters into one with
            Turkish letters."""
            for i in range(len(self.turkish_string)):
                  c = self.turkish_string[i]
                  if self.turkish_need_correction(c, point = i):
                        #self.turkish_string[i] = turkish_toggle_accent(c)
                        #self.turkish_string = self.turkish_string + self.turkish_toggle_accent(c)
                        self.turkish_string = self.set_char_at(self.turkish_string, i, self.turkish_toggle_accent(c))
                  else:
                        #self.turkish_string[i] = c
                        #self.turkish_string = self.turkish_string + c
                        self.turkish_string = self.set_char_at(self.turkish_string, i, c)

            return self.turkish_string


      def turkish_toggle_accent(self, c):
            turkish_toggle_accent_table = {
                 u'c': u'ç',
                 u'C': u'Ç',
                 u'g': u'ğ',
                 u'G': u'Ğ',
                 u'o': u'ö',
                 u'O': u'Ö',
                 u'u': u'ü',
                 u'U': u'Ü',
                 u'i': u'ı',
                 u'I': u'İ',
                 u's': u'ş',
                 u'S': u'Ş',
                 u'ç': u'c',
                 u'Ç': u'C',
                 u'ğ': u'g',
                 u'Ğ': u'G',
                 u'ö': u'o',
                 u'Ö': u'O',
                 u'ü': u'u',
                 u'Ü': u'U',
                 u'ı': u'i',
                 u'İ': u'I',
                 u'ş': u's',
                 u'Ş': u'S'
                 }
            return turkish_toggle_accent_table.get(c, c)


      def turkish_need_correction(self, char, point = 0):
            """Determine if char at cursor needs correction."""
            ch = char
            tr = Deasciifier.turkish_asciify_table.get(ch, ch)
            pl = Deasciifier.turkish_pattern_table.get(tr.lower(),
                                                       False)
            if pl != False:
                  m = self.turkish_match_pattern(pl, point)
            else:
                  m = False

            if tr == u'I':
                  if ch == tr:
                        return not m
                  else:
                        return m
            else:
                  if ch == tr:
                        return m
                  else:
                        return not m

      def turkish_match_pattern(self, dlist, point = 0):
            """Check if the pattern is in the pattern table.
            """
            rank = 2 * len(dlist)
            str = self.turkish_get_context(Deasciifier.turkish_context_size,
                                           point = point)
            start = 0
            end = 0

            _len = len(str)
            while start &lt;= Deasciifier.turkish_context_size:
                  end = 1 + Deasciifier.turkish_context_size
                  while end &lt;= _len:
                        s = str[start:end]
                        r = dlist.get(s, False)
                        if r and abs(r) &lt; abs(rank):
                              rank = r
                        end = 1 + end
                  start = 1 + start

            return rank &gt; 0


      def turkish_get_context(self, size = turkish_context_size, point = 0):
            s = ' ' * (1 + (2 * size))
            s = s[0:size] + 'X' + s[size+1:]
            i = 1 + size
            space = False
            index = point
            current_char = self.turkish_string[index]

            index = index + 1

            while i &lt; len(s) and not space and index &lt; len(self.ascii_string):
                  current_char = self.turkish_string[index]
                  x = Deasciifier.turkish_downcase_asciify_table.get(current_char
                                                                     ,False)
                  if not x:
                        if not space:
                              i = i + 1
                              space = True
                  else:
                        s = s[0:i] + x + s[i+1:]
                        i = i + 1
                        space = False
                  index = index + 1

            s = s[0:i]

            index = point
            i = size - 1
            space = False

            index = index - 1
            while i &gt;= 0 and index &gt;= 0:
                  current_char = self.turkish_string[index]
                  x = Deasciifier.turkish_upcase_accents_table.get(current_char
                                                                   ,False)
                  if not x:
                        if not space:
                              i = i - 1
                              space = True
                  else:
                        s = s[0:i] + x + s[i+1:]
                        i = i - 1
                        space = False
                  index = index - 1

            return s


  </file>

  <file path="./vnlp_colab/ner/ner_colab.py">
# coding=utf-8
#
# Copyright 2025 VNLP Project Authors.
#
# Licensed under the GNU Affero General Public License, Version 3.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.gnu.org/licenses/agpl-3.0.html
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Named Entity Recognition (NER) module for VNLP Colab.

This module contains the high-level NamedEntityRecognizer API and the underlying
model implementations (SPUContextNER, CharNER), refactored for Keras 3,
performance, and a modern developer experience in Colab.
"""
import logging
import pickle
from typing import List, Tuple, Dict, Any, Union

import numpy as np
import sentencepiece as spm
import tensorflow as tf
from tensorflow import keras

# Updated imports for package structure
from vnlp_colab.utils_colab import download_resource, load_keras_tokenizer, get_vnlp_cache_dir
from vnlp_colab.ner.ner_utils_colab import (
    create_spucontext_ner_model, process_ner_input,
    create_charner_model, ner_to_displacy_format
)
from vnlp_colab.tokenizer_colab import WordPunctTokenize


logger = logging.getLogger(__name__)

# --- Model &amp; Resource Configuration ---
_MODEL_CONFIGS = {
    'SPUContextNER': {
        'weights_prod': ("NER_SPUContext_prod.weights", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/NER_SPUContext_prod.weights"),
        'weights_eval': ("NER_SPUContext_eval.weights", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/NER_SPUContext_eval.weights"),
        'word_embedding_matrix': ("SPUTokenized_word_embedding_16k.matrix", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/SPUTokenized_word_embedding_16k.matrix"),
        'spu_tokenizer': ("SPU_word_tokenizer_16k.model", "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/vnlp/resources/SPU_word_tokenizer_16k.model"),
        'label_tokenizer': ("NER_label_tokenizer.json", "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/vnlp/named_entity_recognizer/resources/NER_label_tokenizer.json"),
        'params': {
            'word_embedding_dim': 128,
            'num_rnn_stacks': 2,
            'rnn_units_multiplier': 2,
            'fc_units_multiplier': (2, 1),
            'dropout': 0.2,
        }
    },
    'CharNER': {
        'weights_prod': ("NER_CharNER_prod.weights", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/NER_CharNER_prod.weights"),
        'weights_eval': ("NER_CharNER_eval.weights", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/NER_CharNER_eval.weights"),
        'char_tokenizer': ("CharNER_char_tokenizer.json", "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/vnlp/named_entity_recognizer/resources/CharNER_char_tokenizer.json"),
        'label_tokenizer': ("NER_label_tokenizer.json", "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/vnlp/named_entity_recognizer/resources/NER_label_tokenizer.json"),
        'params': {
            'char_vocab_size': 150,
            'seq_len_max': 256,
            'embed_size': 32,
            'rnn_dim': 128,
            'num_rnn_stacks': 5,
            'mlp_dim': 32,
            'num_classes': 5,
            'dropout': 0.3,
            'padding_strat': 'post',
        }
    }
}

# --- Singleton Caching for Model Instances ---
_MODEL_INSTANCE_CACHE: Dict[str, Any] = {}


class SPUContextNER:
    """
    SentencePiece Unigram Context Named Entity Recognizer.
    Optimized with tf.function for high-performance inference.
    """
    def __init__(self, evaluate: bool = False):
        logger.info(f"Initializing SPUContextNER model (evaluate={evaluate})...")
        config = _MODEL_CONFIGS['SPUContextNER']
        cache_dir = get_vnlp_cache_dir()

        # Download and load resources
        weights_file, weights_url = config['weights_eval'] if evaluate else config['weights_prod']
        weights_path = download_resource(weights_file, weights_url, cache_dir)
        embedding_path = download_resource(*config['word_embedding_matrix'], cache_dir)
        spu_tokenizer_path = download_resource(*config['spu_tokenizer'], cache_dir)
        label_tokenizer_path = download_resource(*config['label_tokenizer'], cache_dir)

        self.spu_tokenizer_word = spm.SentencePieceProcessor(model_file=str(spu_tokenizer_path))
        self.tokenizer_label = load_keras_tokenizer(label_tokenizer_path)
        self._label_index_word = {i: w for w, i in self.tokenizer_label.word_index.items()}
        
        word_embedding_matrix = np.load(embedding_path)
        params = config['params']
        num_rnn_units = params['word_embedding_dim'] * params['rnn_units_multiplier']
        
        self.model = create_spucontext_ner_model(
            vocab_size=self.spu_tokenizer_word.get_piece_size(),
            entity_vocab_size=len(self.tokenizer_label.word_index),
            word_embedding_dim=params['word_embedding_dim'],
            word_embedding_matrix=np.zeros_like(word_embedding_matrix),
            num_rnn_units=num_rnn_units,
            num_rnn_stacks=params['num_rnn_stacks'],
            fc_units_multiplier=params['fc_units_multiplier'],
            dropout=params['dropout']
        )

        with open(weights_path, 'rb') as fp:
            model_weights = pickle.load(fp)
        
        full_weights = [word_embedding_matrix] + model_weights
        self.model.set_weights(full_weights)

        self._initialize_compiled_predict_step()
        logger.info("SPUContextNER model initialized successfully.")

    def _initialize_compiled_predict_step(self):
        """Creates a compiled TensorFlow function for the model's forward pass."""
        entity_vocab_size = len(self.tokenizer_label.word_index)
        input_signature = [
            tf.TensorSpec(shape=(1, 8), dtype=tf.int32),
            tf.TensorSpec(shape=(1, 40, 8), dtype=tf.int32),
            tf.TensorSpec(shape=(1, 40, 8), dtype=tf.int32),
            tf.TensorSpec(shape=(1, 40, entity_vocab_size + 1), dtype=tf.float32),
        ]

        @tf.function(input_signature=input_signature)
        def predict_step(word, left_ctx, right_ctx, lc_entity_history):
            return self.model([word, left_ctx, right_ctx, lc_entity_history], training=False)
            
        self.compiled_predict_step = predict_step

    def predict(self, sentence: str, tokens: List[str], displacy_format: bool = False) -&gt; List[Tuple[str, str]]:
        """
        High-level API for Named Entity Recognition.

        Args:
            sentence (string): Input Sentence (Not used for parsing, but for displacy_format)
            tokens (list): Input sentence tokens.
        Returns:
            List[Tuple[str, str]]: A list of tuple of tokens and NER results 
        """
        if not tokens:
            return []

        num_tokens = len(tokens)
        int_preds: List[int] = []

        for t in range(num_tokens):
            inputs_np = process_ner_input(
                t, tokens, self.spu_tokenizer_word, self.tokenizer_label, int_preds
            )
            inputs_tf = [tf.convert_to_tensor(arr) for arr in inputs_np]
            logits = self.compiled_predict_step(*inputs_tf).numpy()[0]
            int_pred = np.argmax(logits, axis=-1)
            int_preds.append(int_pred)

        preds = [self._label_index_word.get(p, 'O') for p in int_preds]
        ner_result = list(zip(tokens, preds))

        return ner_to_displacy_format(sentence, ner_result) if displacy_format else ner_result

# The CharNER class remains largely unchanged as its architecture is simpler.
# We will ensure its resources are downloaded and managed correctly.
class CharNER:
    """Character-Level Named Entity Recognizer."""
    def __init__(self, evaluate: bool = False):
        logger.info(f"Initializing CharNER model (evaluate={evaluate})...")
        config = _MODEL_CONFIGS['CharNER']
        cache_dir = get_vnlp_cache_dir()

        weights_file, weights_url = config['weights_eval'] if evaluate else config['weights_prod']
        weights_path = download_resource(weights_file, weights_url, cache_dir)
        char_tokenizer_path = download_resource(*config['char_tokenizer'], cache_dir)
        label_tokenizer_path = download_resource(*config['label_tokenizer'], cache_dir)

        self.tokenizer_char = load_keras_tokenizer(char_tokenizer_path)
        self.tokenizer_label = load_keras_tokenizer(label_tokenizer_path)
        
        params = config['params']
        self.model = create_charner_model(
            params['char_vocab_size'], params['embed_size'], params['seq_len_max'],
            params['num_rnn_stacks'], params['rnn_dim'], params['mlp_dim'],
            params['num_classes'], params['dropout']
        )
        
        with open(weights_path, 'rb') as fp:
            self.model.set_weights(pickle.load(fp))

        self.seq_len_max = params['seq_len_max']
        self.padding_strat = params['padding_strat']
        logger.info("CharNER model initialized successfully.")

    def _predict_char_level(self, tokens: List[str]) -&gt; np.ndarray:
        text = " ".join(tokens)
        char_sequence = list(text)
        sequences = self.tokenizer_char.texts_to_sequences([char_sequence])
        padded = keras.preprocessing.sequence.pad_sequences(
            sequences, maxlen=self.seq_len_max, padding=self.padding_strat
        )
        # Use compiled function for single prediction
        raw_pred = self.model(padded, training=False)
        return np.argmax(raw_pred, axis=2).flatten()

    def _charner_decoder(self, tokens: List[str], preds: np.ndarray) -&gt; List[str]:
        lens = [0] + [len(token) + 1 for token in tokens]
        cumsum_of_lens = np.cumsum(lens)
        decoded_entities = []
        for i in range(len(cumsum_of_lens) - 1):
            island = preds[cumsum_of_lens[i] : cumsum_of_lens[i+1] - 1]
            mode_value = 0 # Default for empty tokens
            if island.size &gt; 0:
                vals, counts = np.unique(island, return_counts=True)
                mode_value = vals[np.argmax(counts)]
            detokenized = self.tokenizer_label.sequences_to_texts([[mode_value]])[0]
            decoded_entities.append(detokenized or 'O')
        return decoded_entities

    def predict(self, sentence: str, tokens: List[str], displacy_format: bool = False) -&gt; List[Tuple[str, str]]:
        # CharNER's logic is based on WordPunctTokenize and character lengths, so it re-tokenizes internally.
        # This is an exception to the "tokenize once" rule due to the model's specific design.
        internal_tokens = WordPunctTokenize(sentence)
        
        if len(" ".join(internal_tokens)) &gt; self.seq_len_max:
            # Recursive splitting for long sentences            
            mid = len(internal_tokens) // 2
            first_half_sent = " ".join(internal_tokens[:mid])
            second_half_sent = " ".join(internal_tokens[mid:])
            return self.predict(first_half_sent, []) + self.predict(second_half_sent, [])

        char_preds = self._predict_char_level(internal_tokens)
        decoded_entities = self._charner_decoder(internal_tokens, char_preds)
        ner_result = list(zip(internal_tokens, decoded_entities))
        
        return ner_to_displacy_format(sentence, ner_result) if displacy_format else ner_result


class NamedEntityRecognizer:
    """
    Main API class for Named Entity Recognizer implementations.
    Uses a singleton factory for efficient model instance management.
    """
    def __init__(self, model: str = 'SPUContextNER', evaluate: bool = False):
        self.available_models = ['SPUContextNER', 'CharNER']
        if model not in self.available_models:
            raise ValueError(f"'{model}' is not a valid model. Try one of {self.available_models}")
        self.model_name = model

        cache_key = f"ner_{model}_{'eval' if evaluate else 'prod'}"
        if cache_key not in _MODEL_INSTANCE_CACHE:
            logger.info(f"Instance for '{cache_key}' not found in cache. Creating new one.")
            if model == 'SPUContextNER':
                _MODEL_INSTANCE_CACHE[cache_key] = SPUContextNER(evaluate)
            elif model == 'CharNER':
                _MODEL_INSTANCE_CACHE[cache_key] = CharNER(evaluate)
        else:
            logger.info(f"Found cached instance for '{cache_key}'.")
        self.instance: Union[SPUContextNER, CharNER] = _MODEL_INSTANCE_CACHE[cache_key]

    def predict(self, sentence: str, tokens: List[str], displacy_format: bool = False) -&gt; List[Tuple[str, str]]:
        """High-level API for Named Entity Recognition."""
        return self.instance.predict(sentence, tokens, displacy_format)



# --- Main Entry Point for Standalone Use ---

def main():
    """Demonstrates and tests the NER module functions."""
    from utils_colab import setup_logging
    setup_logging()

    logger.info("--- VNLP Colab NER Test Suite ---")

    # Test SPUContextNER
    try:
        logger.info("\n1. Testing SPUContextNER...")
        ner_spu = NamedEntityRecognizer(model='SPUContextNER')
        sentence1 = "Benim adım Melikşah, İstanbul'da yaşıyorum."
        result1 = ner_spu.predict(sentence1)
        logger.info(f"   Input: '{sentence1}'")
        logger.info(f"   Output: {result1}")
        assert len(result1) &gt; 0 and result1[2][1] == 'PER'
        logger.info("   SPUContextNER test PASSED.")
    except Exception as e:
        logger.error(f"   SPUContextNER test FAILED: {e}", exc_info=True)

    # Test CharNER
    try:
        logger.info("\n2. Testing CharNER...")
        ner_char = NamedEntityRecognizer(model='CharNER')
        sentence2 = "VNGRS AI Takımı'nda çalışıyorum."
        result2 = ner_char.predict(sentence2)
        logger.info(f"   Input: '{sentence2}'")
        logger.info(f"   Output: {result2}")
        assert len(result2) &gt; 0 and result2[0][1] == 'ORG'
        logger.info("   CharNER test PASSED.")
    except Exception as e:
        logger.error(f"   CharNER test FAILED: {e}", exc_info=True)
        
    # Test Singleton Caching
    logger.info("\n3. Testing Singleton Caching...")
    import time
    start_time = time.time()
    ner_spu_cached = NamedEntityRecognizer(model='SPUContextNER')
    end_time = time.time()
    logger.info(f"   Re-initialization took: {end_time - start_time:.4f} seconds.")
    assert (end_time - start_time) &lt; 0.1, "Caching failed, re-initialization was too slow."
    logger.info("   Singleton Caching test PASSED.")


if __name__ == "__main__":
    main()
  </file>

  <file path="./vnlp_colab/ner/ner_utils_colab.py">
# coding=utf-8
#
# Copyright 2025 VNLP Project Authors.
#
# Licensed under the GNU Affero General Public License, Version 3.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.gnu.org/licenses/agpl-3.0.html
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Keras 3 compliant utilities for the SPUContext Named Entity Recognizer (NER).

This module provides the modernized model creation function for the NER model,
ensuring compatibility with the latest TensorFlow/Keras versions and Colab.
"""
import logging
import re
from typing import List, Tuple, Dict

import numpy as np
import tensorflow as tf
from tensorflow import keras

# Updated import for package structure
from vnlp_colab.utils_colab import create_rnn_stacks, process_word_context


logger = logging.getLogger(__name__)

# --- Model Hyperparameters (as constants for clarity) ---
TOKEN_PIECE_MAX_LEN: int = 8
SENTENCE_MAX_LEN: int = 40

# --- SPUContextNER Utilities ---

def create_spucontext_ner_model(
    vocab_size: int,
    entity_vocab_size: int,
    word_embedding_dim: int,
    word_embedding_matrix: np.ndarray,
    num_rnn_units: int,
    num_rnn_stacks: int,
    fc_units_multiplier: tuple[int, int],
    dropout: float
) -&gt; keras.Model:
    """
    Builds the SPUContext NER model using the Keras 3 Functional API.

    This architecture is a 1:1 replica of the original model blueprint, ensuring
    weight compatibility while adhering to modern Keras standards.

    Args:
        vocab_size (int): Vocabulary size for the word embedding layer.
        entity_vocab_size (int): Vocabulary size for the NER entity tags.
            The output layer will have `entity_vocab_size + 1` units.
        word_embedding_dim (int): Dimension of the word embeddings.
        word_embedding_matrix (np.ndarray): Pre-trained word embedding matrix.
        num_rnn_units (int): Number of units in the GRU layers.
        num_rnn_stacks (int): Number of layers in each RNN stack.
        fc_units_multiplier (tuple[int, int]): Multipliers for the dense layers.
        dropout (float): Dropout rate.

    Returns:
        keras.Model: The compiled Keras model.
    """
    logger.info("Creating Keras 3 SPUContext NER model...")

    # --- 1. Define Functional API Inputs ---
    word_input = keras.Input(shape=(TOKEN_PIECE_MAX_LEN,), name='word_input', dtype='int32')
    left_context_input = keras.Input(shape=(SENTENCE_MAX_LEN, TOKEN_PIECE_MAX_LEN), name='left_context_input', dtype='int32')
    right_context_input = keras.Input(shape=(SENTENCE_MAX_LEN, TOKEN_PIECE_MAX_LEN), name='right_context_input', dtype='int32')
    lc_entity_input = keras.Input(shape=(SENTENCE_MAX_LEN, entity_vocab_size + 1), name='lc_entity_input', dtype='float32')

    # --- 2. Define Reusable Sub-Models ---
    word_embedding_layer = keras.layers.Embedding(
        vocab_size, word_embedding_dim,
        embeddings_initializer=keras.initializers.Constant(word_embedding_matrix),
        trainable=False, name='WORD_EMBEDDING'
    )
    word_rnn_stack = create_rnn_stacks(num_rnn_stacks, num_rnn_units, dropout)
    word_rnn_model = keras.Sequential([word_embedding_layer, word_rnn_stack], name="WORD_RNN")

    # --- 3. Build the Four Parallel Branches of the Main Graph ---
    # Branch 1: Current word
    word_output = word_rnn_model(word_input)

    # Branch 2: Left context
    left_context_vectors = keras.layers.TimeDistributed(word_rnn_model)(left_context_input)
    left_context_stack = create_rnn_stacks(num_rnn_stacks, num_rnn_units, dropout)
    left_context_output = left_context_stack(left_context_vectors)

    # Branch 3: Right context
    right_context_vectors = keras.layers.TimeDistributed(word_rnn_model)(right_context_input)
    right_context_stack = create_rnn_stacks(num_rnn_stacks, num_rnn_units, dropout, go_backwards=True)
    right_context_output = right_context_stack(right_context_vectors)

    # Branch 4: Previously predicted (left) NER tags
    lc_entity_stack = create_rnn_stacks(num_rnn_stacks, num_rnn_units, dropout)
    lc_entity_output = lc_entity_stack(lc_entity_input)

    # --- 4. Concatenate Branches and Add Final FC Layers ---
    concatenated = keras.layers.Concatenate()([word_output, left_context_output, right_context_output, lc_entity_output])

    x = keras.layers.Dense(num_rnn_units * fc_units_multiplier[0], activation='relu')(concatenated)
    x = keras.layers.Dropout(dropout)(x)
    x = keras.layers.Dense(num_rnn_units * fc_units_multiplier[1], activation='relu')(x)
    x = keras.layers.Dropout(dropout)(x)
    entity_output = keras.layers.Dense(entity_vocab_size + 1, activation='softmax')(x)
    
    # --- 5. Build and Return the Final Model ---
    ner_model = keras.Model(
        inputs=[word_input, left_context_input, right_context_input, lc_entity_input],
        outputs=entity_output, name='SPUContext_NER_Model'
    )
    logger.info("SPUContext NER model created successfully.")
    return ner_model

def process_ner_input(
    word_index: int,
    sentence_tokens: List[str],
    spu_tokenizer_word: 'spm.SentencePieceProcessor',
    ner_label_tokenizer: tf.keras.preprocessing.text.Tokenizer,
    previous_predictions: List[int]
) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Prepares input arrays for a single word for the NER model.

    This function vectorizes the word and its context, as well as the history of
    previously predicted entity tags for the autoregressive loop.

    Args:
        word_index (int): Index of the current word in the sentence.
        sentence_tokens (List[str]): List of all tokens in the sentence.
        spu_tokenizer_word: The SentencePiece tokenizer.
        ner_label_tokenizer: The Keras tokenizer for NER labels.
        previous_predictions (List[int]): A list of integer NER tag predictions
            for the preceding words in the sentence.

    Returns:
        A tuple of NumPy arrays ready for model input:
        (current_word, left_context, right_context, left_entity_history)
    """
    entity_vocab_size = len(ner_label_tokenizer.word_index) + 1

    # 1. Process word and its context
    current_word, left_context, right_context = process_word_context(
        word_index, sentence_tokens, spu_tokenizer_word, SENTENCE_MAX_LEN, TOKEN_PIECE_MAX_LEN
    )
    
    # 2. Create the history of previous NER tag predictions    
    left_entity_history = np.zeros((SENTENCE_MAX_LEN, entity_vocab_size), dtype=np.float32)
    if word_index &gt; 0:
        one_hot_preds = tf.keras.utils.to_categorical(previous_predictions, num_classes=entity_vocab_size)
        start_pos = SENTENCE_MAX_LEN - word_index
        left_entity_history[start_pos : start_pos + len(one_hot_preds)] = one_hot_preds
    
    # 3. Expand dims to create a batch of 1 for model prediction
    return (
        np.expand_dims(current_word, axis=0),
        np.expand_dims(left_context, axis=0),
        np.expand_dims(right_context, axis=0),
        np.expand_dims(left_entity_history, axis=0)
    )

# --- CharNER Utilities (Consolidated) ---

def create_charner_model(
    char_vocab_size: int, embed_size: int, seq_len_max: int,
    num_rnn_stacks: int, rnn_dim: int, mlp_dim: int,
    num_classes: int, dropout: float
) -&gt; keras.Model:
    """Builds the Character-Level NER model."""
    logger.info("Creating Keras 3 CharNER model...")
    model = keras.Sequential(name="CharNER_Model")
    model.add(keras.layers.Input(shape=(seq_len_max,), dtype='int32'))
    model.add(keras.layers.Embedding(input_dim=char_vocab_size, output_dim=embed_size))

    for _ in range(num_rnn_stacks):
        model.add(keras.layers.Bidirectional(keras.layers.GRU(rnn_dim, return_sequences=True)))
        model.add(keras.layers.Dropout(dropout))

    model.add(keras.layers.Dense(mlp_dim, activation='relu'))
    model.add(keras.layers.Dropout(dropout))
    model.add(keras.layers.Dense(num_classes, activation='softmax'))
    
    logger.info("CharNER model created successfully.")
    return model

def ner_to_displacy_format(text: str, ner_result: List[Tuple[str, str]]) -&gt; Dict:
    """Converts NER results to the dictionary format for spacy.displacy."""
    displacy_data = {'text': text, 'ents': [], 'title': None}
    current_entity = None
    entity_text = ""
    start_char = 0

    # Find character spans for each token
    token_spans = []
    search_start = 0
    for token, _ in ner_result:
        start = text.find(token, search_start)
        if start != -1:
            end = start + len(token)
            token_spans.append((start, end))
            search_start = end
        else:
            token_spans.append(None) # Could not find token

    # Group consecutive entities
    for i, ((token, entity), span) in enumerate(zip(ner_result, token_spans)):
        if span is None: continue

        if entity != 'O':
            if current_entity is None:
                # Start of a new entity
                current_entity = entity
                entity_text = token
                start_char = span[0]
            elif entity == current_entity:
                # Continuation of the current entity
                entity_text += text[token_spans[i-1][1]:span[0]] + token # Add whitespace and token
            else:
                # End of the previous entity, start of a new one
                end_char = token_spans[i-1][1]
                displacy_data['ents'].append({'start': start_char, 'end': end_char, 'label': current_entity})
                current_entity = entity
                entity_text = token
                start_char = span[0]
        elif current_entity is not None:
            # End of an entity span
            end_char = token_spans[i-1][1]
            displacy_data['ents'].append({'start': start_char, 'end': end_char, 'label': current_entity})
            current_entity = None
            entity_text = ""

    # Add the last entity if the sentence ends with one
    if current_entity is not None:
        end_char = token_spans[-1][1]
        displacy_data['ents'].append({'start': start_char, 'end': end_char, 'label': current_entity})

    return displacy_data
  </file>

  <file path="./vnlp_colab/tokenizer_colab.py">
# coding=utf-8
#
# Copyright 2025 VNLP Project Authors.
#
# Licensed under the GNU Affero General Public License, Version 3.0 (the "License");
# ... (license text) ...
"""
Tokenization utilities for VNLP Colab, providing Treebank and WordPunct tokenizers.
"""
import re
from typing import List

def WordPunctTokenize(text: str) -&gt; List[str]:
    """
    Splits text into a sequence of alphabetic and non-alphabetic characters.
    A simplified version of NLTK's WordPunctTokenizer.
    """
    pattern = r"\w+|[^\w\s]+"
    return re.findall(pattern, text, flags=re.UNICODE | re.MULTILINE | re.DOTALL)

def TreebankWordTokenize(text: str) -&gt; List[str]:
    """
    Tokenizes text with rules that preserve hyphens and apostrophes inside words.
    """
    # This regex handles words with internal hyphens/apostrophes, plain words, and punctuation.
    tokens = re.findall(r"\b\w+(?:[-’/']\w+)+\b|\w+|[^\w\s]", text)
    # Normalize unicode apostrophe to ASCII for consistency
    return [t.replace("’", "'") for t in tokens]
  </file>

  <file path="./vnlp_colab/utils_colab.py">
# coding=utf-8
#
# Copyright 2025 VNLP Project Authors.
#
# Licensed under the GNU Affero General Public License, Version 3.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.gnu.org/licenses/agpl-3.0.html
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Core utilities for the VNLP package, refactored for Google Colab.

This module provides essential functions for resource management (downloading,
caching), environment setup (hardware detection, logging), and modern Keras 3 /
TensorFlow compatibility. It is designed to be the backbone of the vnlp_colab
package, ensuring efficient and maintainable execution.

- Version: 1.0.0
- Keras: 3.x
- TensorFlow: 2.16+
- Python: 3.10+
"""

import logging
import os
from pathlib import Path
from typing import Dict, List, Tuple, Union

import numpy as np
import requests
import tensorflow as tf
from tqdm.notebook import tqdm

# --- Environment and Logging Setup ---

__version__ = "1.0.0"
logger = logging.getLogger(__name__)

def setup_logging(level: int = logging.INFO) -&gt; None:
    """Configures structured logging for the VNLP package."""
    logging.basicConfig(
        level=level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    # Suppress verbose TensorFlow warnings for a cleaner output
    os.environ.setdefault('TF_CPP_MIN_LOG_LEVEL', '2')
    tf.get_logger().setLevel('ERROR')
    logger.info("VNLP logging configured.")
    logger.info(f"Running on TensorFlow v{tf.__version__} and Keras v{tf.keras.__version__}")

def get_vnlp_cache_dir() -&gt; Path:
    """
    Returns the Path object for the VNLP cache directory.

    Defaults to /content/vnlp_cache in a Colab-like environment.
    Creates the directory if it does not exist.

    Returns:
        Path: The path to the cache directory.
    """
    cache_dir = Path("/content/vnlp_cache")
    try:
        cache_dir.mkdir(parents=True, exist_ok=True)
    except OSError as e:
        logger.error(f"Failed to create cache directory at {cache_dir}: {e}")
        raise
    return cache_dir

def detect_hardware_strategy() -&gt; tf.distribute.Strategy:
    """
    Detects and returns the appropriate TensorFlow distribution strategy.

    - TPUStrategy for TPUs.
    - MirroredStrategy for multiple GPUs.
    - OneDeviceStrategy for a single GPU or CPU.

    Returns:
        tf.distribute.Strategy: The detected distribution strategy.
    """
    try:
        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
        logger.info(f'Running on TPU {tpu.master()}')
        tf.config.experimental_connect_to_cluster(tpu)
        tf.tpu.experimental.initialize_tpu_system(tpu)
        return tf.distribute.TPUStrategy(tpu)
    except (ValueError, tf.errors.NotFoundError):
        logger.info("TPU not found. Checking for GPUs.")

    gpus = tf.config.list_physical_devices('GPU')
    if not gpus:
        logger.info("No GPUs found. Using CPU strategy.")
        return tf.distribute.OneDeviceStrategy(device="/cpu:0")

    if len(gpus) &gt; 1:
        logger.info(f"Multiple GPUs found ({len(gpus)}). Using MirroredStrategy.")
        return tf.distribute.MirroredStrategy()

    logger.info("Single GPU found. Using OneDeviceStrategy.")
    # Set memory growth to avoid allocating all GPU memory at once
    for gpu in gpus:
        try:
            tf.config.experimental.set_memory_growth(gpu, True)
        except RuntimeError as e:
            logger.warning(f"Could not set memory growth for GPU {gpu.name}: {e}")
    return tf.distribute.OneDeviceStrategy(device="/gpu:0")

# --- Resource Management ---

def download_resource(
    file_name: str,
    file_url: str,
    cache_dir: Union[str, Path, None] = None,
    overwrite: bool = False
) -&gt; Path:
    """
    Checks if a file exists and downloads it if not, with a progress bar.

    Args:
        file_name (str): The name of the file to save.
        file_url (str): The URL to download the file from.
        cache_dir (Union[str, Path, None], optional): Directory to cache the file.
            Defaults to the standard VNLP cache.
        overwrite (bool, optional): If True, re-downloads the file even if it exists.
            Defaults to False.

    Returns:
        Path: The local path to the downloaded file.

    Raises:
        requests.exceptions.RequestException: If the download fails.
    """
    if cache_dir is None:
        cache_dir = get_vnlp_cache_dir()
    else:
        cache_dir = Path(cache_dir)
        cache_dir.mkdir(parents=True, exist_ok=True)

    file_path = cache_dir / file_name

    if file_path.exists() and not overwrite:
        logger.info(f"'{file_name}' already exists at '{file_path}'. Skipping download.")
        return file_path

    logger.info(f"Downloading '{file_name}' from '{file_url}'...")
    try:
        with requests.get(file_url, stream=True) as response:
            response.raise_for_status()
            # Use .get('content-length', '0') to safely handle missing header
            total_size = int(response.headers.get('content-length', 0))
            block_size = 1024  # 1 KB

            with open(file_path, 'wb') as f, tqdm(
                total=total_size,
                unit='iB',
                unit_scale=True,
                desc=f"Downloading {file_name}",
                disable=total_size == 0 # Disable bar if total size is unknown
            ) as progress_bar:
                for data in response.iter_content(block_size):
                    progress_bar.update(len(data))
                    f.write(data)

            # --- FIX: Only warn if content-length was provided and doesn't match ---
            if total_size &gt; 0 and progress_bar.n != total_size:
                logger.warning(f"Download of '{file_name}' might be incomplete. "
                               f"Expected {total_size} bytes, got {progress_bar.n} bytes.")

    except requests.exceptions.RequestException as e:
        logger.error(f"Failed to download '{file_name}': {e}")
        # Clean up partial download
        if file_path.exists():
            file_path.unlink()
        raise

    logger.info(f"Download of '{file_name}' completed successfully to '{file_path}'.")
    return file_path

def load_keras_tokenizer(tokenizer_json_path: Union[str, Path]) -&gt; tf.keras.preprocessing.text.Tokenizer:
    """
    Loads a Keras tokenizer from a JSON file.

    Args:
        tokenizer_json_path (Union[str, Path]): Path to the tokenizer JSON file.

    Returns:
        tf.keras.preprocessing.text.Tokenizer: The loaded tokenizer object.
    """
    logger.info(f"Loading Keras tokenizer from: {tokenizer_json_path}")
    try:
        with open(tokenizer_json_path, 'r', encoding='utf-8') as f:
            tokenizer_json = f.read()
        return tf.keras.preprocessing.text.tokenizer_from_json(tokenizer_json)
    except FileNotFoundError:
        logger.error(f"Tokenizer file not found at: {tokenizer_json_path}")
        raise
    except Exception as e:
        logger.error(f"Failed to load tokenizer from {tokenizer_json_path}: {e}")
        raise


# --- Keras 3 &amp; TensorFlow Model Utilities ---

def create_rnn_stacks(
    num_rnn_stacks: int,
    num_rnn_units: int,
    dropout: float,
    go_backwards: bool = False
) -&gt; tf.keras.Model:
    """
    Creates a stack of GRU layers with dropout, compatible with Keras 3.

    Args:
        num_rnn_stacks (int): The total number of GRU layers in the stack.
        num_rnn_units (int): The number of units in each GRU layer.
        dropout (float): The dropout rate to apply between layers.
        go_backwards (bool, optional): If True, process sequences in reverse.
            Defaults to False.

    Returns:
        tf.keras.Model: A Sequential model containing the stack of GRU layers.
    """
    if num_rnn_stacks &lt; 1:
        raise ValueError("num_rnn_stacks must be at least 1.")

    rnn_layers = []
    # All but the last layer should return sequences
    for _ in range(num_rnn_stacks - 1):
        rnn_layers.append(
            tf.keras.layers.GRU(
                num_rnn_units,
                dropout=dropout,
                return_sequences=True,
                go_backwards=go_backwards
            )
        )
    # The last layer returns only the final state
    rnn_layers.append(
        tf.keras.layers.GRU(
            num_rnn_units,
            dropout=dropout,
            return_sequences=False,
            go_backwards=go_backwards
        )
    )
    return tf.keras.Sequential(rnn_layers)


def tokenize_single_word(
    word: str,
    tokenizer_word: 'spm.SentencePieceProcessor',
    token_piece_max_len: int
) -&gt; np.ndarray:
    """
    Tokenizes and pads a single word using a SentencePiece tokenizer.

    Args:
        word (str): The input word.
        tokenizer_word (spm.SentencePieceProcessor): The SentencePiece tokenizer instance.
        token_piece_max_len (int): The maximum length for padding/truncating.

    Returns:
        np.ndarray: A 1D NumPy array of token IDs.
    """
    tokenized_ids = tokenizer_word.encode_as_ids(word)
    padded = tf.keras.preprocessing.sequence.pad_sequences(
        [tokenized_ids],
        maxlen=token_piece_max_len,
        padding='pre',
        truncating='pre'
    )
    return padded[0]

def process_word_context(
    word_index: int,
    sentence_tokens: List[str],
    tokenizer_word: 'spm.SentencePieceProcessor',
    sentence_max_len: int,
    token_piece_max_len: int
) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:
    """
    Processes the context (left, current, right) for a single word in a sentence.

    Args:
        word_index (int): Index of the current word in the sentence.
        sentence_tokens (List[str]): The list of all tokens in the sentence.
        tokenizer_word: The SentencePiece tokenizer instance.
        sentence_max_len (int): The max length of the context window on each side.
        token_piece_max_len (int): The max length of token pieces for a single word.

    Returns:
        Tuple[np.ndarray, np.ndarray, np.ndarray]: A tuple containing the processed
            current word, left context, and right context as NumPy arrays.
    """
    current_word = sentence_tokens[word_index]
    left_context = sentence_tokens[:word_index]
    right_context = sentence_tokens[word_index + 1:]

    # Process current word
    current_word_processed = tokenize_single_word(
        current_word, tokenizer_word, token_piece_max_len
    ).astype(np.int32)

    # Process left context
    left_context_processed = np.array([
        tokenize_single_word(w, tokenizer_word, token_piece_max_len) for w in left_context
    ], dtype=np.int32)
    
    # Pre-pad and truncate left context
    num_left_pad = sentence_max_len - len(left_context_processed)
    if num_left_pad &gt; 0:
        padding = np.zeros((num_left_pad, token_piece_max_len), dtype=np.int32)
        left_context_processed = np.vstack((padding, left_context_processed))
    elif num_left_pad &lt; 0:
        left_context_processed = left_context_processed[-sentence_max_len:]

    # Process right context
    right_context_processed = np.array([
        tokenize_single_word(w, tokenizer_word, token_piece_max_len) for w in right_context
    ], dtype=np.int32)

    # Post-pad and truncate right context
    num_right_pad = sentence_max_len - len(right_context_processed)
    if num_right_pad &gt; 0:
        padding = np.zeros((num_right_pad, token_piece_max_len), dtype=np.int32)
        right_context_processed = np.vstack((right_context_processed, padding))
    elif num_right_pad &lt; 0:
        right_context_processed = right_context_processed[:sentence_max_len]

    return current_word_processed, left_context_processed, right_context_processed

# --- Main Entry Point for Standalone Use ---

def main() -&gt; None:
    """
    Main function to demonstrate and test the utility functions.
    This can be executed as a standalone script.
    """
    setup_logging()
    logger.info("--- VNLP Colab Utilities Test Suite ---")

    # 1. Test Hardware Detection
    logger.info("\n1. Testing Hardware Detection:")
    strategy = detect_hardware_strategy()
    logger.info(f"   Detected Strategy: {strategy.__class__.__name__}")

    # 2. Test Caching and Downloading
    logger.info("\n2. Testing Caching and Downloading:")
    test_file_url = "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/LICENSE"
    test_file_name = "LICENSE_test.txt"
    try:
        # First download
        license_path = download_resource(test_file_name, test_file_url)
        logger.info(f"   Successfully downloaded to: {license_path}")
        # Second (cached) download
        license_path_cached = download_resource(test_file_name, test_file_url)
        logger.info(f"   Cached path: {license_path_cached}")
    except Exception as e:
        logger.error(f"   Download test failed: {e}")

    # 3. Test Usage Snippet
    logger.info("\n3. Example Usage Snippet:")
    print("\nfrom utils_colab import setup_logging, download_resource, detect_hardware_strategy")
    print("setup_logging()")
    print("strategy = detect_hardware_strategy()")
    print("# model = load_model(strategy) # In your model loading script")
    print("# print(preprocess_text('Sample input for VNLP'))")

if __name__ == "__main__":
    main()
  </file>

  <file path="./vnlp_colab/stemmer/stemmer_utils_colab.py">
# coding=utf-8
#
# Copyright 2025 VNLP Project Authors.
#
# Licensed under the GNU Affero General Public License, Version 3.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.gnu.org/licenses/agpl-3.0.html
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Keras 3 compliant utilities for the Stemmer/Morphological Analyzer.

THIS IS THE FINAL, CORRECTED VERSION. It abandons the incorrect blueprint and
builds the true architecture based on the original code's logic, which matches
the 36 weight tensors in the saved weights file.
"""
import logging
from typing import List, Tuple, Dict, Any

import numpy as np
import tensorflow as tf
from tensorflow import keras

logger = logging.getLogger(__name__)

def create_stemmer_model(
    num_max_analysis: int,
    stem_max_len: int,
    char_vocab_size: int,
    char_embed_size: int,
    stem_num_rnn_units: int,
    tag_max_len: int,
    tag_vocab_size: int,
    tag_embed_size: int,
    tag_num_rnn_units: int,
    sentence_max_len: int,
    surface_token_max_len: int,
    embed_join_type: str = 'add',
    dropout: float = 0.2,
    num_rnn_stacks: int = 1,
    **kwargs # Accept other params to avoid breaking the call
) -&gt; keras.Model:
    """
    Builds the stemmer model using the Keras 3 Functional API. This is the
    correct architecture with 36 weight tensors.
    """
    logger.info("Creating Keras 3 Stemmer model (Final Corrected Architecture)...")
    surface_num_rnn_units = stem_num_rnn_units + tag_num_rnn_units

    # --- 1. Define Inputs ---
    stem_input = keras.Input(shape=(num_max_analysis, stem_max_len), dtype='int32', name='stem_input')
    tag_input = keras.Input(shape=(num_max_analysis, tag_max_len), dtype='int32', name='tag_input')
    surface_left_input = keras.Input(shape=(sentence_max_len, surface_token_max_len), dtype='int32', name='surface_left_input')
    surface_right_input = keras.Input(shape=(sentence_max_len, surface_token_max_len), dtype='int32', name='surface_right_input')

    # --- 2. Define Shared Layers &amp; Reusable Sub-Models ---
    char_embedding = keras.layers.Embedding(char_vocab_size, char_embed_size, name='char_embedding')
    tag_embedding = keras.layers.Embedding(tag_vocab_size, tag_embed_size, name='tag_embedding')

    stem_rnn = keras.Sequential([
        keras.layers.Bidirectional(keras.layers.GRU(stem_num_rnn_units)),
        keras.layers.Dropout(dropout)
    ], name='stem_char_rnn')

    tag_rnn = keras.Sequential([
        keras.layers.Bidirectional(keras.layers.GRU(tag_num_rnn_units)),
        keras.layers.Dropout(dropout)
    ], name='tag_char_rnn')

    # Create two INDEPENDENT RNNs for surface form processing to match the weight count
    surface_char_rnn_left = keras.Sequential([
        keras.layers.Bidirectional(keras.layers.GRU(surface_num_rnn_units)),
        keras.layers.Dropout(dropout)
    ], name='surface_char_rnn_left')

    surface_char_rnn_right = keras.Sequential([
        keras.layers.Bidirectional(keras.layers.GRU(surface_num_rnn_units)),
        keras.layers.Dropout(dropout)
    ], name='surface_char_rnn_right')

    # --- 3. Build "R" Component (Analysis Representation) ---
    stem_embedded = char_embedding(stem_input)
    td_stem_rnn = keras.layers.TimeDistributed(stem_rnn)(stem_embedded)

    tag_embedded = tag_embedding(tag_input)
    td_tag_rnn = keras.layers.TimeDistributed(tag_rnn)(tag_embedded)

    joined_stem_tag = keras.layers.Add()([td_stem_rnn, td_tag_rnn])
    R = keras.layers.Activation('tanh')(joined_stem_tag)

    # --- 4. Build "h" Component (Context Representation) ---
    surface_embedded_left = char_embedding(surface_left_input)
    td_surface_left = keras.layers.TimeDistributed(surface_char_rnn_left)(surface_embedded_left)
    surface_left_context = keras.layers.GRU(surface_num_rnn_units)(td_surface_left)

    surface_embedded_right = char_embedding(surface_right_input)
    td_surface_right = keras.layers.TimeDistributed(surface_char_rnn_right)(surface_embedded_right)
    surface_right_context = keras.layers.GRU(surface_num_rnn_units, go_backwards=True)(td_surface_right)

    joined_context = keras.layers.Add()([surface_left_context, surface_right_context])
    h = keras.layers.Activation('tanh')(joined_context)

    # --- 5. Final Combination and Output ---
    p = keras.layers.Dot(axes=(2, 1))([R, h])
    p = keras.layers.Dense(num_max_analysis * 2, activation='tanh')(p)
    p = keras.layers.Dropout(dropout)(p)
    p = keras.layers.Dense(num_max_analysis, activation='softmax')(p)

    model = keras.Model(
        inputs=[stem_input, tag_input, surface_left_input, surface_right_input],
        outputs=p,
        name='StemmerDisambiguationModel_Final'
    )
    return model


def process_stemmer_input(
    data: List[Tuple[List[str], List[List[Dict[str, Any]]]]],
    tokenizer_char: tf.keras.preprocessing.text.Tokenizer,
    tokenizer_tag: tf.keras.preprocessing.text.Tokenizer,
    stem_max_len: int,
    tag_max_len: int,
    surface_token_max_len: int,
    sentence_max_len: int,
    num_max_analysis: int
) -&gt; Tuple[Tuple[np.ndarray, ...], np.ndarray]:
    """Processes raw data into NumPy arrays for the model."""
    all_tokens_data = []
    for sentence_tokens, sentence_analyses in data:
        for i in range(len(sentence_tokens)):
            all_tokens_data.append({
                "analyses": sentence_analyses[i],
                "left_context": sentence_tokens[max(0, i - sentence_max_len):i],
                "right_context": sentence_tokens[i + 1 : i + 1 + sentence_max_len]
            })

    num_total_tokens = len(all_tokens_data)
    if num_total_tokens == 0:
        return (np.array([]), np.array([]), np.array([]), np.array([])), np.array([])

    stems_batch = np.zeros((num_total_tokens, num_max_analysis, stem_max_len), dtype=np.int32)
    tags_batch = np.zeros((num_total_tokens, num_max_analysis, tag_max_len), dtype=np.int32)
    labels_batch = np.zeros((num_total_tokens, num_max_analysis), dtype=np.int32)
    left_context_batch = np.zeros((num_total_tokens, sentence_max_len, surface_token_max_len), dtype=np.int32)
    right_context_batch = np.zeros((num_total_tokens, sentence_max_len, surface_token_max_len), dtype=np.int32)

    for i, token_data in enumerate(all_tokens_data):
        stem_candidates = [analysis[0] for analysis in token_data["analyses"]]
        tag_candidates_as_lists = [analysis[2] for analysis in token_data["analyses"]]
        tag_candidates_as_strings = [' '.join(tags) for tags in tag_candidates_as_lists]

        tokenized_stems = tokenizer_char.texts_to_sequences(stem_candidates)
        padded_stems = keras.preprocessing.sequence.pad_sequences(
            tokenized_stems, maxlen=stem_max_len, padding='pre', truncating='pre'
        )

        tokenized_tags = tokenizer_tag.texts_to_sequences(tag_candidates_as_strings)
        padded_tags = keras.preprocessing.sequence.pad_sequences(
            tokenized_tags, maxlen=tag_max_len, padding='pre', truncating='pre'
        )
        
        num_analyses = min(len(padded_stems), num_max_analysis)
        stems_batch[i, :num_analyses] = padded_stems[:num_analyses]
        tags_batch[i, :num_analyses] = padded_tags[:num_analyses]
        
        if num_analyses &gt; 0:
            labels_batch[i, 0] = 1

        if token_data["left_context"]:
            tokenized_left = tokenizer_char.texts_to_sequences(token_data["left_context"])
            padded_left = keras.preprocessing.sequence.pad_sequences(
                tokenized_left, maxlen=surface_token_max_len, padding='pre', truncating='pre'
            )
            left_context_batch[i, -len(padded_left):] = padded_left
        
        if token_data["right_context"]:
            tokenized_right = tokenizer_char.texts_to_sequences(token_data["right_context"])
            padded_right = keras.preprocessing.sequence.pad_sequences(
                tokenized_right, maxlen=surface_token_max_len, padding='pre', truncating='post'
            )
            right_context_batch[i, :len(padded_right)] = padded_right

    inputs = (stems_batch, tags_batch, left_context_batch, right_context_batch)
    return inputs, labels_batch
  </file>

  <file path="./vnlp_colab/stemmer/stemmer_colab.py">
# coding=utf-8
#
# Copyright 2025 VNLP Project Authors.
#
# Licensed under the GNU Affero General Public License, Version 3.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.gnu.org/licenses/agpl-3.0.html
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Stemmer and Morphological Analyzer module for VNLP Colab.

This module contains the high-level StemmerAnalyzer API, refactored for
Keras 3, high-performance inference, and a modern developer experience.
"""
import logging
import pickle
from typing import List, Tuple, Dict, Any

import numpy as np
import tensorflow as tf
from tensorflow import keras

# Updated imports for package structure
from vnlp_colab.utils_colab import download_resource, load_keras_tokenizer, get_vnlp_cache_dir
from vnlp_colab.stemmer.stemmer_utils_colab import create_stemmer_model, process_stemmer_input
from vnlp_colab.stemmer._yildiz_analyzer import get_candidate_generator_instance, capitalize

logger = logging.getLogger(__name__)

# --- Model &amp; Resource Configuration ---
_MODEL_CONFIGS = {
    'StemmerAnalyzer': {
        'weights_prod': ("Stemmer_Shen_prod.weights", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/Stemmer_Shen_prod.weights"),
        'weights_eval': ("Stemmer_Shen_eval.weights", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/Stemmer_Shen_eval.weights"),
        'char_tokenizer': ("Stemmer_char_tokenizer.json", "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/vnlp/stemmer_morph_analyzer/resources/Stemmer_char_tokenizer.json"),
        'tag_tokenizer': ("Stemmer_morph_tag_tokenizer.json", "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/vnlp/stemmer_morph_analyzer/resources/Stemmer_morph_tag_tokenizer.json"),
        'params': {
            'num_max_analysis': 10, 'stem_max_len': 10, 'tag_max_len': 15,
            'sentence_max_len': 40, 'surface_token_max_len': 15,
            'char_embed_size': 32, 'tag_embed_size': 32,
            'stem_num_rnn_units': 128, 'tag_num_rnn_units': 128,
            'num_rnn_stacks': 1, 'dropout': 0.2, 'embed_join_type': 'add',
            'capitalize_pnons': False,
        }
    }
}

# --- Singleton Caching for Model Instances ---
_MODEL_INSTANCE_CACHE: Dict[str, Any] = {}


class StemmerAnalyzer:
    """
    High-level API for Morphological Disambiguation.

    This class uses a singleton factory pattern for efficient instance management.
    The underlying model is an implementation of "The Role of Context in Neural
    Morphological Disambiguation", optimized for Keras 3 and Colab.
    """
# In vnlp_colab/stemmer/stemmer_colab.py

    def __init__(self, evaluate: bool = False):
        """
        Initializes the model, loads all necessary resources, and compiles the
        prediction function. This is a heavyweight operation managed by the
        singleton factory.
        """
        logger.info(f"Initializing StemmerAnalyzer model (evaluate={evaluate})...")
        config = _MODEL_CONFIGS['StemmerAnalyzer']
        self.params = config['params']
        cache_dir = get_vnlp_cache_dir()

        # --- FIX: Isolate the post-processing parameter ---
        self.capitalize_pnons = self.params.pop('capitalize_pnons', False)

        # --- Download and Load Resources ---
        weights_file, weights_url = config['weights_eval'] if evaluate else config['weights_prod']
        weights_path = download_resource(weights_file, weights_url, cache_dir)
        char_tokenizer_path = download_resource(*config['char_tokenizer'], cache_dir)
        tag_tokenizer_path = download_resource(*config['tag_tokenizer'], cache_dir)

        self.tokenizer_char = load_keras_tokenizer(char_tokenizer_path)
        self.tokenizer_tag = load_keras_tokenizer(tag_tokenizer_path)
        self.tokenizer_tag.filters = ''
        self.tokenizer_tag.split = ' '

        char_vocab_size = len(self.tokenizer_char.word_index) + 1
        tag_vocab_size = len(self.tokenizer_tag.word_index) + 1

        # --- Build and Load Model ---
        # The **self.params call is now safe because 'capitalize_pnons' has been removed.
        self.model = create_stemmer_model(
            char_vocab_size=char_vocab_size,
            tag_vocab_size=tag_vocab_size,
            **self.params
        )
        
        with open(weights_path, 'rb') as fp:
            self.model.set_weights(pickle.load(fp))

        self.candidate_generator = get_candidate_generator_instance(case_sensitive=True)
        self._initialize_compiled_predict_step()
        logger.info("StemmerAnalyzer model initialized successfully.")

    def _initialize_compiled_predict_step(self):
        """Creates a compiled TensorFlow function for a faster forward pass."""
        p = self.params
        input_signature = [
            tf.TensorSpec(shape=(None, p['num_max_analysis'], p['stem_max_len']), dtype=tf.int32),
            tf.TensorSpec(shape=(None, p['num_max_analysis'], p['tag_max_len']), dtype=tf.int32),
            tf.TensorSpec(shape=(None, p['sentence_max_len'], p['surface_token_max_len']), dtype=tf.int32),
            tf.TensorSpec(shape=(None, p['sentence_max_len'], p['surface_token_max_len']), dtype=tf.int32),
        ]

        @tf.function(input_signature=input_signature)
        def predict_step(stems, tags, left_ctx, right_ctx):
            return self.model([stems, tags, left_ctx, right_ctx], training=False)
            
        self.compiled_predict_step = predict_step

    def predict(self, tokens: List[str], batch_size: int = 64) -&gt; List[str]:
        """High-level API for Morphological Disambiguation on pre-tokenized text.
        
        Args:
            tokens (list): Input sentence tokens.
            batch_size (int): Number of tokens to process in one model call.

        Returns:
            List[str]: A list of the selected morphological analyses.
        """
        if not tokens:
            return []

        sentence_analyses = [self.candidate_generator.get_analysis_candidates(token) for token in tokens]
        data_for_processing = [(tokens, sentence_analyses)]
        
        x_numpy, _ = process_stemmer_input(
            data_for_processing, self.tokenizer_char, self.tokenizer_tag,
            stem_max_len=self.params['stem_max_len'],
            tag_max_len=self.params['tag_max_len'],
            surface_token_max_len=self.params['surface_token_max_len'],
            sentence_max_len=self.params['sentence_max_len'],
            num_max_analysis=self.params['num_max_analysis']
        )
        
        if len(tokens) &lt;= batch_size:
            probs = self.compiled_predict_step(*x_numpy)
        else:
            all_probs = []
            for i in range(0, len(tokens), batch_size):
                batch_x = tuple(x[i : i + batch_size] for x in x_numpy)
                all_probs.append(self.compiled_predict_step(*batch_x))
            probs = tf.concat(all_probs, axis=0)

        ambig_levels = np.array([len(a) for a in sentence_analyses], dtype=np.int32)
        mask = tf.sequence_mask(ambig_levels, maxlen=self.params['num_max_analysis'], dtype=tf.float32)
        
        predicted_indices = tf.argmax(probs * mask, axis=-1).numpy()

        final_result = []
        for i, analyses in enumerate(sentence_analyses):
            pred_idx = predicted_indices[i]
            if pred_idx &lt; len(analyses):
                root, _, tags = analyses[pred_idx]
                # --- FIX: Use the instance attribute directly ---
                if "Prop" in tags and self.capitalize_pnons:
                    root = capitalize(root)
                analysis_str = "+".join([root] + tags).replace('+DB', '^DB')
                final_result.append(analysis_str)
            else:
                final_result.append(tokens[i] + "+Unknown")

        return final_result


def get_stemmer_analyzer(evaluate: bool = False) -&gt; StemmerAnalyzer:
    """Singleton factory function for the StemmerAnalyzer."""
    cache_key = f"stemmer_{'eval' if evaluate else 'prod'}"
    if cache_key not in _MODEL_INSTANCE_CACHE:
        _MODEL_INSTANCE_CACHE[cache_key] = StemmerAnalyzer(evaluate=evaluate)
    return _MODEL_INSTANCE_CACHE[cache_key]
  </file>

  <file path="./vnlp_colab/stemmer/_yildiz_analyzer.py">
# File: /home/ben/miniconda3/envs/bookanalysis/lib/python3.12/site-packages/vnlp/stemmer_morph_analyzer/_yildiz_analyzer.py
# ---
# This file has been audited and corrected. The original, behaviorally-correct
# utility functions have been restored to fix an accuracy regression.

# -*- coding: utf-8 -*-
import re
import os
import logging
from collections import namedtuple

resources_path = os.path.join(os.path.dirname(__file__), "resources/")

_GENERATOR_INSTANCE_CACHE = {}

def get_candidate_generator_instance(case_sensitive=True, asciification=False, suffix_normalization=False):
    """Singleton factory for TurkishStemSuffixCandidateGenerator."""
    cache_key = (case_sensitive, asciification, suffix_normalization)
    if cache_key not in _GENERATOR_INSTANCE_CACHE:
        instance = TurkishStemSuffixCandidateGenerator(case_sensitive, asciification, suffix_normalization)
        _GENERATOR_INSTANCE_CACHE[cache_key] = instance
    return _GENERATOR_INSTANCE_CACHE[cache_key]

class TurkishStemSuffixCandidateGenerator(object):
    """
    Generates morphological analysis candidates for Turkish words.
    This version uses a Singleton pattern for initialization and memoization
    for the analysis function to significantly improve performance.
    """
    ROOT_TRANSFORMATION_MAP = {"tıp": "tıb", "prof.": "profesör", "dr.": "doktor", "yi": "ye", "ed": "et", "di": "de"}
    TAG_FLAG_MAP = {0: "Adj", 1: "Adverb", 2: "Conj", 3: "Det", 4: "Dup", 5: "Interj", 6: "Noun", 7: "Postp", 8: "Pron", 9: "Ques", 10: "Verb", 11: "Num", 12: "Noun+Prop"}
    SUFFIX_DICT_FILE_PATH = os.path.join(resources_path, "Suffixes&amp;Tags.txt")
    STEM_LIST_FILE_PATH = os.path.join(resources_path, "StemListWithFlags_v2.txt")
    EXACT_LOOKUP_TABLE_FILE_PATH = os.path.join(resources_path, "ExactLookup.txt")
    CONSONANT_STR = "[bcdfgğhjklmnprsştvyzxwqBCDFGĞHJKLMNPRSŞTVYZXWQ]"
    VOWEL_STR = "[aeıioöuüAEIİOÖUÜ]"
    NARROW_VOWELS_STR = "[uüıiUÜIİ]"
    STARTS_WITH_UPPER = re.compile(r"^[ABCÇDEFGĞHIİJKLMNOÖPRSŞTUÜQVYXZ].*$")
    ENDS_WITH_SOFT_CONSONANTS_REGEX = re.compile(r"^.*[bcdğBCDĞgG]$")
    SUFFIX_TRANSFORMATION_REGEX1 = re.compile(r"[ae]")
    SUFFIX_TRANSFORMATION_REGEX2 = re.compile(r"[ıiuü]")
    ENDS_TWO_CONSONANT_REG = re.compile(r"^.*{}{}$".format(CONSONANT_STR, CONSONANT_STR))
    STARTS_VOWEL_REGEX = re.compile(r"^{}.*$".format(VOWEL_STR))
    ENDS_NARROW_REGEX = re.compile(r"^.*{}$".format(NARROW_VOWELS_STR))
    TAG_SEPARATOR_REGEX = re.compile(r"[\+\^]")
    NON_WORD_REGEX = re.compile(r"^[^A-Za-zışğüçöÜĞİŞÇÖ]+$")
    CONTAINS_NUMBER_REGEX = re.compile(r"^.*[0-9].*$")

    def __init__(self, case_sensitive=True, asciification=False, suffix_normalization=False):
        self.case_sensitive = case_sensitive
        self.asciification = asciification
        self.suffix_normalization = suffix_normalization
        self.suffix_dic = self._read_suffix_dic()
        self.stem_dic = self._read_stem_list()
        self.exact_lookup_table = self._read_exact_lookup_table()
        self.analysis_cache = {}

    def _read_exact_lookup_table(self):
        table = {}
        with open(self.EXACT_LOOKUP_TABLE_FILE_PATH, "r", encoding="UTF-8") as f:
            for line in f:
                splits = line.strip().split("\t")
                table[splits[0]] = splits[1].split(" ")
        return table

    def _read_suffix_dic(self):
        dic = {}
        with open(self.SUFFIX_DICT_FILE_PATH, "r", encoding="UTF-8") as f:
            for line in f:
                suffix, tag = line.strip().split("\t")
                if suffix not in dic:
                    dic[suffix] = []
                dic[suffix].append(tag)
        return dic

    def _read_stem_list(self):
        dic = {}
        with open(self.STEM_LIST_FILE_PATH, "r", encoding="UTF-8") as f:
            for line in f:
                stem, flag_str = line.strip().split("\t")
                if not self.case_sensitive:
                    stem = to_lower(stem)
                postags = self._parse_flag(int(flag_str.strip()))
                if stem in dic:
                    dic[stem].extend(p for p in postags if p not in dic[stem])
                else:
                    dic[stem] = postags
        return dic

    @staticmethod
    def _parse_flag(flag):
        res = []
        for i in range(12, -1, -1):
            power_of_2 = 1 &lt;&lt; i
            if flag &gt;= power_of_2:
                res.append(TurkishStemSuffixCandidateGenerator.TAG_FLAG_MAP[i])
                flag -= power_of_2
        if flag != 0:
            raise IOError("Error: problems in stem flags!")
        return res

    @staticmethod
    def _transform_soft_consonants(text):
        replacements = [
            (r"^(.*)b$", r"\1p"), (r"^(.*)B$", r"\1P"), (r"^(.*)c$", r"\1ç"),
            (r"^(.*)C$", r"\1Ç"), (r"^(.*)d$", r"\1t"), (r"^(.*)D$", r"\1T"),
            (r"^(.*)ğ$", r"\1k"), (r"^(.*)Ğ$", r"\1K"), (r"^(.*)g$", r"\1k"),
            (r"^(.*)G$", r"\1K")
        ]
        for pattern, repl in replacements:
            text = re.sub(pattern, repl, text)
        return text

    @staticmethod
    def _root_transform(candidate_roots):
        for i, root in enumerate(candidate_roots):
            if root in TurkishStemSuffixCandidateGenerator.ROOT_TRANSFORMATION_MAP:
                candidate_roots[i] = TurkishStemSuffixCandidateGenerator.ROOT_TRANSFORMATION_MAP[root]

    @classmethod
    def suffix_transform(cls, candidate_suffixes):
        for i, suffix in enumerate(candidate_suffixes):
            candidate_suffixes[i] = cls.suffix_transform_single(suffix)

    @classmethod
    def suffix_transform_single(cls, candidate_suffix):
        candidate_suffix = to_lower(candidate_suffix)
        candidate_suffix = cls.SUFFIX_TRANSFORMATION_REGEX1.sub("A", candidate_suffix)
        candidate_suffix = cls.SUFFIX_TRANSFORMATION_REGEX2.sub("H", candidate_suffix)
        return candidate_suffix

    @staticmethod
    def _add_candidate_stem_suffix(stem_candidate, suffix_candidate, candidate_roots, candidate_suffixes):
        # This is a complex rule-based method that remains unchanged from the original.
        if "'" in suffix_candidate: candidate_roots.append(stem_candidate); candidate_suffixes.append(suffix_candidate); return
        if stem_candidate == "ban" and suffix_candidate == "a": candidate_roots.append("ben"); candidate_suffixes.append("a")
        elif stem_candidate == "Ban" and suffix_candidate == "a": candidate_roots.append("Ben"); candidate_suffixes.append("a")
        elif stem_candidate == "san" and suffix_candidate == "a": candidate_roots.append("sen"); candidate_suffixes.append("a")
        elif stem_candidate == "San" and suffix_candidate == "a": candidate_roots.append("Sen"); candidate_suffixes.append("a")
        else:
            candidate_roots.append(stem_candidate); candidate_suffixes.append(suffix_candidate)
            if len(stem_candidate) &gt; 2 and len(suffix_candidate) &gt; 0 and stem_candidate[-1] == suffix_candidate[0] and stem_candidate[-1] in TurkishStemSuffixCandidateGenerator.CONSONANT_STR: candidate_roots.append(stem_candidate); candidate_suffixes.append(suffix_candidate[1:])
            elif len(stem_candidate) &gt; 1 and TurkishStemSuffixCandidateGenerator.ENDS_NARROW_REGEX.match(stem_candidate) and "yor" in suffix_candidate:
                if stem_candidate.endswith("i") or stem_candidate.endswith("ü"): candidate_roots.append(stem_candidate[:-1] + "e"); candidate_suffixes.append(suffix_candidate)
                elif stem_candidate.endswith("ı") or stem_candidate.endswith("u"): candidate_roots.append(stem_candidate[:-1] + "a"); candidate_suffixes.append(suffix_candidate)
            if len(stem_candidate) &gt; 2 and TurkishStemSuffixCandidateGenerator.ENDS_TWO_CONSONANT_REG.match(stem_candidate) and TurkishStemSuffixCandidateGenerator.STARTS_VOWEL_REGEX.match(suffix_candidate):
                suffix_start_letter = to_lower(suffix_candidate[0])
                if suffix_start_letter in ["u", "ü", "ı", "i"]: candidate_roots.append(stem_candidate[:-1] + suffix_start_letter + stem_candidate[-1]); candidate_suffixes.append(suffix_candidate)
                elif suffix_start_letter == "e": candidate_roots.append(stem_candidate[:-1] + "i" + stem_candidate[-1]); candidate_suffixes.append(suffix_candidate); candidate_roots.append(stem_candidate[:-1] + "ü" + stem_candidate[-1]); candidate_suffixes.append(suffix_candidate)
                elif suffix_start_letter == "a": candidate_roots.append(stem_candidate[:-1] + "ı" + stem_candidate[-1]); candidate_suffixes.append(suffix_candidate); candidate_roots.append(stem_candidate[:-1] + "u" + stem_candidate[-1]); candidate_suffixes.append(suffix_candidate)
            if len(stem_candidate) &gt; 2 and TurkishStemSuffixCandidateGenerator.ENDS_WITH_SOFT_CONSONANTS_REGEX.match(stem_candidate): candidate_roots.append(TurkishStemSuffixCandidateGenerator._transform_soft_consonants(stem_candidate)); candidate_suffixes.append(suffix_candidate)

    def get_stem_suffix_candidates(self, surface_word):
        candidate_roots, candidate_suffixes = [], []
        for i in range(1, len(surface_word)):
            candidate_root, candidate_suffix = surface_word[:i], surface_word[i:]
            if not self.case_sensitive:
                self._add_candidate_stem_suffix(to_lower(candidate_root), to_lower(candidate_suffix), candidate_roots, candidate_suffixes)
            else:
                lower_suffix = to_lower(candidate_suffix)
                self._add_candidate_stem_suffix(to_lower(candidate_root), lower_suffix, candidate_roots, candidate_suffixes)
                if self.STARTS_WITH_UPPER.match(candidate_root): self._add_candidate_stem_suffix(capitalize(candidate_root), lower_suffix, candidate_roots, candidate_suffixes)
        candidate_suffixes.append(""), candidate_roots.append(to_lower(surface_word))
        if self.case_sensitive and self.STARTS_WITH_UPPER.match(surface_word): candidate_suffixes.append(""), candidate_roots.append(capitalize(surface_word))
        self._root_transform(candidate_roots)
        if self.asciification: candidate_roots, candidate_suffixes = [asciify(r) for r in candidate_roots], [asciify(s) for s in candidate_suffixes]
        if self.suffix_normalization: self.suffix_transform(candidate_suffixes)
        return candidate_roots, candidate_suffixes

    def get_tags(self, suffix, stem_tags=None):
        tags = self.suffix_dic.get(suffix)
        if not tags:
            if suffix.startswith("'") and suffix[1:] in self.suffix_dic:
                tags = self.suffix_dic[suffix[1:]]
            elif not suffix: # Handles "null" suffix case
                tags = self.suffix_dic["null"]
            else:
                return []
        res = []
        for tag in set(tags):
            tag_sequences = self.TAG_SEPARATOR_REGEX.split(tag)
            first_tag = "+".join(tag_sequences[0:2]) if len(tag_sequences) &gt; 1 and tag_sequences[1] in ["Prop", "Time"] else tag_sequences[0]
            if not stem_tags or first_tag in stem_tags:
                res.append(tag_sequences)
        return res

    def get_analysis_candidates(self, surface_word):
        if surface_word in self.analysis_cache:
            return self.analysis_cache[surface_word]

        surface_word_lower = to_lower(surface_word)
        if surface_word_lower in self.exact_lookup_table:
            analyses = []
            for analysis_str in self.exact_lookup_table[surface_word_lower]:
                root, tags_str = analysis_str.split("/")
                root_part = self.TAG_SEPARATOR_REGEX.split(tags_str)[0]
                tags = self.TAG_SEPARATOR_REGEX.split(tags_str)[1:]
                analyses.append((root_part, root, tags))
            self.analysis_cache[surface_word] = analyses
            return analyses

        candidate_analyzes, candidate_analyzes_str = [], set()
        candidate_roots, candidate_suffixes = self.get_stem_suffix_candidates(surface_word)

        for c_root, c_suffix in zip(candidate_roots, candidate_suffixes):
            if self.NON_WORD_REGEX.match(c_root):
                stem_tags = ["Num", "Noun+Time"] if self.CONTAINS_NUMBER_REGEX.match(c_root) else ["Punc"]
            elif c_root not in self.stem_dic:
                if len(c_suffix) == 0: continue
                stem_tags = ["Noun+Prop"] if "'" in c_suffix and c_suffix in self.suffix_dic else []
                if not stem_tags: continue
            else:
                stem_tags = list(self.stem_dic[c_root])
                is_upper = self.STARTS_WITH_UPPER.match(c_root)
                has_prop = "Noun+Prop" in stem_tags
                if not is_upper and has_prop: stem_tags.remove("Noun+Prop")
                elif is_upper and has_prop: stem_tags = ["Noun+Prop"]
                elif c_suffix.startswith("'") and has_prop: stem_tags = ["Noun+Prop"]
                elif is_upper and not has_prop: continue

            for candidate_tag in self.get_tags(c_suffix, stem_tags):
                analysis_str = to_lower(c_root) + "+" + "+".join(candidate_tag).replace("+DB", "^DB")
                if analysis_str not in candidate_analyzes_str:
                    candidate_analyzes.append((to_lower(c_root), c_suffix, candidate_tag))
                    candidate_analyzes_str.add(analysis_str)
        
        if not candidate_analyzes:
            candidate_analyzes.append((surface_word_lower, "", ["Unknown"]))
        
        self.analysis_cache[surface_word] = candidate_analyzes
        return candidate_analyzes

def to_lower(text):
    text = text.replace("İ", "i")
    text = text.replace("I", "ı")
    text = text.replace("Ğ", "ğ")
    text = text.replace("Ü", "ü")
    text = text.replace("Ö", "ö")
    text = text.replace("Ş", "ş")
    text = text.replace("Ç", "ç")
    return text.lower()

def capitalize(text):
    if len(text) &gt; 1:
        return asciify(text[0]).upper() + to_lower(text[1:])
    else:
        return text

def asciify(text):
    text = text.replace("İ", "I")
    text = text.replace("Ç", "C")
    text = text.replace("Ğ", "G")
    text = text.replace("Ü", "U")
    text = text.replace("Ş", "S")
    text = text.replace("Ö", "O")
    text = text.replace("ı", "i")
    text = text.replace("ç", "c")
    text = text.replace("ğ", "g")
    text = text.replace("ü", "u")
    text = text.replace("ş", "s")
    text = text.replace("ö", "o")
    return text
  </file>

  <file path="./vnlp_colab/sentiment/sentiment_utils_colab.py">
# coding=utf-8
#
# Copyright 2025 VNLP Project Authors.
#
# Licensed under the GNU Affero General Public License, Version 3.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.gnu.org/licenses/agpl-3.0.html
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Keras 3 compliant utilities for the SPUCBiGRU Sentiment Analyzer.

This module provides the modernized model creation and data processing functions
for the sentiment analysis model.
"""
import logging
from typing import TYPE_CHECKING

import numpy as np
import tensorflow as tf
from tensorflow import keras

if TYPE_CHECKING:
    import sentencepiece as spm

logger = logging.getLogger(__name__)


def create_spucbigru_sentiment_model(
    text_max_len: int,
    vocab_size: int,
    word_embedding_dim: int,
    word_embedding_matrix: np.ndarray,
    num_rnn_units: int,
    num_rnn_stacks: int,
    dropout_rate: float,
) -&gt; keras.Model:
    """
    Creates a Bidirectional GRU model for sentiment analysis using Keras 3 Functional API.

    This architecture is a 1:1 replica of the original model, ensuring
    weight compatibility.

    Args:
        text_max_len (int): Maximum length of the input sequence.
        vocab_size (int): Vocabulary size for the embedding layer.
        word_embedding_dim (int): Dimension of the word embeddings.
        word_embedding_matrix (np.ndarray): Pre-trained word embedding matrix.
        num_rnn_units (int): Number of units in the GRU layers.
        num_rnn_stacks (int): Number of stacked Bidirectional GRU layers.
        dropout_rate (float): Dropout rate for regularization.

    Returns:
        keras.Model: The compiled Keras model for sentiment analysis.
    """
    logger.info("Creating Keras 3 SPUCBiGRU Sentiment model...")
    
    # --- 1. Define Input and Embedding Layers ---
    input_layer = keras.Input(shape=(text_max_len,), name="input_layer", dtype="int32")
    embedding_layer = keras.layers.Embedding(
        input_dim=vocab_size,
        output_dim=word_embedding_dim,
        embeddings_initializer=keras.initializers.Constant(word_embedding_matrix),
        trainable=False,
        name="word_embedding",
    )(input_layer)

    # --- 2. Build Stacked Bidirectional GRU Layers ---
    model_flow = embedding_layer
    for i in range(num_rnn_stacks):
        model_flow = keras.layers.Bidirectional(
            keras.layers.GRU(num_rnn_units, dropout=dropout_rate, return_sequences=True),
            name=f"bidirectional_gru_{i+1}",
        )(model_flow)

    # --- 3. Add Pooling and Final Dense Layers ---
    model_flow = keras.layers.GlobalAveragePooling1D(name="global_avg_pooling")(model_flow)
    model_flow = keras.layers.Dropout(dropout_rate, name="post_pooling_dropout")(model_flow)
    model_flow = keras.layers.Dense(num_rnn_units // 8, activation='relu', name="intermediate_dense")(model_flow)
    model_flow = keras.layers.Dropout(dropout_rate, name="final_dropout")(model_flow)
    output_layer = keras.layers.Dense(1, activation='sigmoid', name="sentiment_output")(model_flow)

    # --- 4. Create and Return the Model ---
    model = keras.models.Model(inputs=input_layer, outputs=output_layer, name="SPUCBiGRU_Sentiment_Model")
    logger.info("SPUCBiGRU Sentiment model created successfully.")
    return model


def process_sentiment_input(
    text: str, tokenizer: "spm.SentencePieceProcessor", text_max_len: int
) -&gt; np.ndarray:
    """
    Tokenizes, truncates, and pads input text for the sentiment model.

    Args:
        text (str): The raw input sentence.
        tokenizer (spm.SentencePieceProcessor): The SentencePiece tokenizer instance.
        text_max_len (int): The maximum sequence length the model expects.

    Returns:
        np.ndarray: A NumPy array of shape (1, text_max_len) ready for model input.
    """
    tokenized_ids = tokenizer.encode_as_ids(text)
    
    # Use Keras's pad_sequences for robust padding and truncation.
    # 'pre' padding and 'post' truncating match the original model's behavior.
    padded_sequence = keras.preprocessing.sequence.pad_sequences(
        [tokenized_ids], maxlen=text_max_len, padding='pre', truncating='post'
    )
    return padded_sequence.astype(np.int32)
  </file>

  <file path="./vnlp_colab/sentiment/sentiment_colab.py">
# coding=utf-8
#
# Copyright 2025 VNLP Project Authors.
#
# Licensed under the GNU Affero General Public License, Version 3.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.gnu.org/licenses/agpl-3.0.html
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Sentiment Analyzer module for VNLP Colab.

This module provides the high-level SentimentAnalyzer API, which uses a
singleton factory to manage the SPUCBiGRUSentimentAnalyzer instance. The
implementation is optimized for Keras 3 and high-performance inference.
"""
import logging
import pickle
from typing import Dict, Any, Optional, Tuple

import numpy as np
import sentencepiece as spm
import tensorflow as tf
from tensorflow import keras

# Updated imports for package structure
from vnlp_colab.utils_colab import download_resource, get_vnlp_cache_dir
from vnlp_colab.sentiment.sentiment_utils_colab import create_spucbigru_sentiment_model, process_sentiment_input

logger = logging.getLogger(__name__)

# --- Model &amp; Resource Configuration ---
_MODEL_CONFIGS = {
    'SPUCBiGRUSentimentAnalyzer': {
        'weights_prod': ("Sentiment_SPUCBiGRU_prod.weights", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/Sentiment_SPUCBiGRU_prod.weights"),
        'weights_eval': ("Sentiment_SPUCBiGRU_eval.weights", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/Sentiment_SPUCBiGRU_eval.weights"),
        'word_embedding_matrix': ("SPUTokenized_word_embedding_16k.matrix", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/SPUTokenized_word_embedding_16k.matrix"),
        'spu_tokenizer': ("SPU_word_tokenizer_16k.model", "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/vnlp/resources/SPU_word_tokenizer_16k.model"),
        'params': {
            'text_max_len': 256,
            'word_embedding_dim': 128,
            'num_rnn_stacks': 3,
            'num_rnn_units': 128,
            'dropout_rate': 0.2,
        }
    }
}

# --- Singleton Caching for Model Instances ---
_MODEL_INSTANCE_CACHE: Dict[str, Any] = {}


class SPUCBiGRUSentimentAnalyzer:
    """
    SentencePiece Unigram Context Bidirectional GRU Sentiment Analyzer.
    Optimized with tf.function for high-performance inference.
    """
    def __init__(self, evaluate: bool = False):
        logger.info(f"Initializing SPUCBiGRUSentimentAnalyzer (evaluate={evaluate})...")
        config = _MODEL_CONFIGS['SPUCBiGRUSentimentAnalyzer']
        self.params = config['params']
        cache_dir = get_vnlp_cache_dir()

        # Download and load resources
        weights_file, weights_url = config['weights_eval'] if evaluate else config['weights_prod']
        weights_path = download_resource(weights_file, weights_url, cache_dir)
        embedding_path = download_resource(*config['word_embedding_matrix'], cache_dir)
        spu_tokenizer_path = download_resource(*config['spu_tokenizer'], cache_dir)

        self.spu_tokenizer_word = spm.SentencePieceProcessor(model_file=str(spu_tokenizer_path))
        word_embedding_matrix = np.load(embedding_path)
        
        vocab_size = self.spu_tokenizer_word.get_piece_size()
        
        self.model = create_spucbigru_sentiment_model(
            vocab_size=vocab_size,
            word_embedding_matrix=np.zeros_like(word_embedding_matrix),
            **self.params
        )
        
        with open(weights_path, 'rb') as fp:
            trainable_weights = pickle.load(fp)

        # The non-trainable embedding matrix is the first weight
        full_weights = [word_embedding_matrix] + trainable_weights
        self.model.set_weights(full_weights)

        self._initialize_compiled_predict_step()
        logger.info("SPUCBiGRUSentimentAnalyzer initialized successfully.")

    def _initialize_compiled_predict_step(self):
        """Creates a compiled TensorFlow function for a faster forward pass."""
        input_signature = [tf.TensorSpec(shape=(None, self.params['text_max_len']), dtype=tf.int32)]

        @tf.function(input_signature=input_signature)
        def inference_function(input_tensor):
            return self.model(input_tensor, training=False)
        
        self.compiled_predict = inference_function

    def predict_proba(self, text: str) -&gt; float:
        """Predicts the sentiment probability for a given text."""
        processed_input = process_sentiment_input(text, self.spu_tokenizer_word, self.params['text_max_len'])
        prob = self.compiled_predict(tf.constant(processed_input)).numpy()[0][0]
        return float(prob)


class SentimentAnalyzer:
    """
    Main API class for Sentiment Analyzer implementations.
    Uses a singleton factory for efficient model instance management.
    """
    def __init__(self, model: str = 'SPUCBiGRUSentimentAnalyzer', evaluate: bool = False):
        self.available_models = ['SPUCBiGRUSentimentAnalyzer']
        if model not in self.available_models:
            raise ValueError(f"'{model}' is not a valid model. Try one of {self.available_models}")

        cache_key = f"sentiment_{model}_{'eval' if evaluate else 'prod'}"
        if cache_key not in _MODEL_INSTANCE_CACHE:
            logger.info(f"Instance for '{cache_key}' not found. Creating new one.")
            if model == 'SPUCBiGRUSentimentAnalyzer':
                _MODEL_INSTANCE_CACHE[cache_key] = SPUCBiGRUSentimentAnalyzer(evaluate)
        else:
            logger.info(f"Found cached instance for '{cache_key}'.")

        self.instance: SPUCBiGRUSentimentAnalyzer = _MODEL_INSTANCE_CACHE[cache_key]

    def predict(self, text: str) -&gt; int:
        """
        Predicts a discrete sentiment label (1 for positive, 0 for negative).
        """
        prob = self.predict_proba(text)
        return 1 if prob &gt; 0.5 else 0

    def predict_proba(self, text: str) -&gt; float:
        """
        Predicts the probability of a positive sentiment.
        """
        return self.instance.predict_proba(text)

# --- Main Entry Point for Standalone Use ---
def main():
    """Demonstrates and tests the Sentiment Analyzer module."""
    from utils_colab import setup_logging
    setup_logging()
    
    logger.info("--- VNLP Colab Sentiment Analyzer Test Suite ---")
    
    # Test SPUCBiGRUSentimentAnalyzer
    try:
        logger.info("\n1. Testing SPUCBiGRUSentimentAnalyzer...")
        sentiment_analyzer = SentimentAnalyzer()
        sentence_pos = "Bu filmi çok beğendim, harikaydı."
        sentence_neg = "Tam bir zaman kaybı, hiç tavsiye etmiyorum."

        prob_pos = sentiment_analyzer.predict_proba(sentence_pos)
        pred_pos = sentiment_analyzer.predict(sentence_pos)
        logger.info(f"   Input: '{sentence_pos}' -&gt; Proba: {prob_pos:.4f}, Pred: {pred_pos}")
        assert pred_pos == 1 and prob_pos &gt; 0.5

        prob_neg = sentiment_analyzer.predict_proba(sentence_neg)
        pred_neg = sentiment_analyzer.predict(sentence_neg)
        logger.info(f"   Input: '{sentence_neg}' -&gt; Proba: {prob_neg:.4f}, Pred: {pred_neg}")
        assert pred_neg == 0 and prob_neg &lt; 0.5
        
        logger.info("   SPUCBiGRUSentimentAnalyzer test PASSED.")
    except Exception as e:
        logger.error(f"   SPUCBiGRUSentimentAnalyzer test FAILED: {e}", exc_info=True)

    # Test Singleton Caching
    logger.info("\n2. Testing Singleton Caching...")
    import time
    start_time = time.time()
    _ = SentimentAnalyzer()
    end_time = time.time()
    logger.info(f"   Re-initialization took: {end_time - start_time:.4f} seconds.")
    assert (end_time - start_time) &lt; 0.1, "Caching failed, re-initialization was too slow."
    logger.info("   Singleton Caching test PASSED.")

if __name__ == "__main__":
    main()
  </file>

  <file path="./vnlp_colab/dep/dep_utils_colab.py">
# coding=utf-8
#
# Copyright 2025 VNLP Project Authors.
#
# Licensed under the GNU Affero General Public License, Version 3.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.gnu.org/licenses/agpl-3.0.html
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Keras 3 compliant utilities for the SPUContext Dependency Parser (DP).

This module provides the modernized model creation function for the DP model,
ensuring compatibility with the latest TensorFlow/Keras versions and Colab.
"""
import logging
from typing import List, Tuple, Dict

import numpy as np
import tensorflow as tf
from tensorflow import keras

# Updated import for package structure
from vnlp_colab.utils_colab import create_rnn_stacks, process_word_context

logger = logging.getLogger(__name__)

# --- Model Hyperparameters (as constants for clarity) ---
TOKEN_PIECE_MAX_LEN: int = 8
SENTENCE_MAX_LEN: int = 40

# --- SPUContextDP Utilities ---

def create_spucontext_dp_model(
    vocab_size: int,
    arc_label_vector_len: int,
    word_embedding_dim: int,
    word_embedding_matrix: np.ndarray,
    num_rnn_units: int,
    num_rnn_stacks: int,
    fc_units_multiplier: tuple[int, int],
    dropout: float
) -&gt; keras.Model:
    """
    Builds the SPUContext Dependency Parser model using the Keras 3 Functional API.

    This architecture is a 1:1 replica of the original model blueprint, ensuring
    weight compatibility while adhering to modern Keras standards.

    Args:
        vocab_size (int): Vocabulary size for the word embedding layer.
        arc_label_vector_len (int): The total length of the concatenated one-hot
            vectors for arc and label predictions.
        word_embedding_dim (int): Dimension of the word embeddings.
        word_embedding_matrix (np.ndarray): Pre-trained word embedding matrix.
        num_rnn_units (int): Number of units in the GRU layers.
        num_rnn_stacks (int): Number of layers in each RNN stack.
        fc_units_multiplier (tuple[int, int]): Multipliers for the dense layers.
        dropout (float): Dropout rate.

    Returns:
        keras.Model: The compiled Keras model.
    """
    logger.info("Creating Keras 3 SPUContext Dependency Parser model...")

    # --- 1. Define Functional API Inputs ---
    word_input = keras.Input(shape=(TOKEN_PIECE_MAX_LEN,), name='word_input', dtype='int32')
    left_context_input = keras.Input(shape=(SENTENCE_MAX_LEN, TOKEN_PIECE_MAX_LEN), name='left_context_input', dtype='int32')
    right_context_input = keras.Input(shape=(SENTENCE_MAX_LEN, TOKEN_PIECE_MAX_LEN), name='right_context_input', dtype='int32')
    lc_arc_label_input = keras.Input(shape=(SENTENCE_MAX_LEN, arc_label_vector_len), name='lc_arc_label_input', dtype='float32')

    # --- 2. Define Reusable Sub-Models ---
    word_embedding_layer = keras.layers.Embedding(
        vocab_size, word_embedding_dim,
        embeddings_initializer=keras.initializers.Constant(word_embedding_matrix),
        trainable=False, name='WORD_EMBEDDING'
    )
    word_rnn_stack = create_rnn_stacks(num_rnn_stacks, num_rnn_units, dropout)
    word_rnn_model = keras.Sequential([word_embedding_layer, word_rnn_stack], name="WORD_RNN")

    # --- 3. Build the Four Parallel Branches of the Main Graph ---
    # Branch 1: Current word
    word_output = word_rnn_model(word_input)

    # Branch 2: Left context
    left_context_vectors = keras.layers.TimeDistributed(word_rnn_model)(left_context_input)
    left_context_stack = create_rnn_stacks(num_rnn_stacks, num_rnn_units, dropout)
    left_context_output = left_context_stack(left_context_vectors)

    # Branch 3: Right context
    right_context_vectors = keras.layers.TimeDistributed(word_rnn_model)(right_context_input)
    right_context_stack = create_rnn_stacks(num_rnn_stacks, num_rnn_units, dropout, go_backwards=True)
    right_context_output = right_context_stack(right_context_vectors)

    # Branch 4: Previously predicted (left) arc-labels
    lc_arc_label_stack = create_rnn_stacks(num_rnn_stacks, num_rnn_units, dropout)
    lc_arc_label_output = lc_arc_label_stack(lc_arc_label_input)

    # --- 4. Concatenate Branches and Add Final FC Layers ---
    concatenated = keras.layers.Concatenate()([word_output, left_context_output, right_context_output, lc_arc_label_output])

    x = keras.layers.Dense(num_rnn_units * fc_units_multiplier[0], activation='relu')(concatenated)
    x = keras.layers.Dropout(dropout)(x)
    x = keras.layers.Dense(num_rnn_units * fc_units_multiplier[1], activation='relu')(x)
    x = keras.layers.Dropout(dropout)(x)
    arc_label_output = keras.layers.Dense(arc_label_vector_len, activation='sigmoid')(x)

    # --- 5. Build and Return the Final Model ---
    dp_model = keras.Model(
        inputs=[word_input, left_context_input, right_context_input, lc_arc_label_input],
        outputs=arc_label_output, name='SPUContext_DP_Model'
    )
    logger.info("SPUContext DP model created successfully.")
    return dp_model

def process_dp_input(
    word_index: int,
    sentence_tokens: List[str],
    spu_tokenizer_word: 'spm.SentencePieceProcessor',
    dp_label_tokenizer: tf.keras.preprocessing.text.Tokenizer,
    arc_label_vector_len: int,
    previous_arcs: List[int],
    previous_labels: List[int]
) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Prepares input arrays for a single word for the DP model.

    Args:
        word_index (int): Index of the current word.
        sentence_tokens (List[str]): All tokens in the sentence.
        spu_tokenizer_word: The SentencePiece tokenizer.
        dp_label_tokenizer: The Keras tokenizer for DP labels.
        arc_label_vector_len (int): Total length of the arc+label vector.
        previous_arcs (List[int]): History of predicted arc indices.
        previous_labels (List[int]): History of predicted label indices.

    Returns:
        A tuple of NumPy arrays ready for model input:
        (current_word, left_context, right_context, left_arc_label_history)
    """
    label_vocab_size = len(dp_label_tokenizer.word_index) + 1

    # 1. Process word and its context    
    current_word, left_context, right_context = process_word_context(
        word_index, sentence_tokens, spu_tokenizer_word, SENTENCE_MAX_LEN, TOKEN_PIECE_MAX_LEN
    )

    # 2. Create the history of previous arc-label predictions
    left_arc_label_history = np.zeros((SENTENCE_MAX_LEN, arc_label_vector_len), dtype=np.float32)
    for prev_idx in range(word_index):
        arc = previous_arcs[prev_idx]
        label = previous_labels[prev_idx]
        arc_vector = tf.keras.utils.to_categorical(arc, num_classes=SENTENCE_MAX_LEN + 1)
        label_vector = tf.keras.utils.to_categorical(label, num_classes=label_vocab_size)
        combined_vector = np.concatenate([arc_vector, label_vector])

        # Ensure vector length matches expected input shape
        if len(combined_vector) != arc_label_vector_len:
            raise ValueError(f"Generated arc-label vector length ({len(combined_vector)}) != expected length ({arc_label_vector_len}).")
        position = (SENTENCE_MAX_LEN - word_index) + prev_idx
        left_arc_label_history[position] = combined_vector

    # 3. Expand dims to create a batch of 1    
    return (
        np.expand_dims(current_word, axis=0),
        np.expand_dims(left_context, axis=0),
        np.expand_dims(right_context, axis=0),
        np.expand_dims(left_arc_label_history, axis=0)
    )

# --- General DP Utilities (Consolidated) ---

def decode_arc_label_vector(
    vector: np.ndarray, sentence_max_len: int, label_vocab_size: int
) -&gt; Tuple[int, int]:
    """Decodes the model's raw output vector into a predicted arc and label."""
    arc_logits = vector[:sentence_max_len + 1]
    arc = np.argmax(arc_logits, axis=-1)
    label_start_index = sentence_max_len + 1
    label_end_index = label_start_index + label_vocab_size + 1
    label_logits = vector[label_start_index:label_end_index]
    label = np.argmax(label_logits, axis=-1)
    return int(arc), int(label)

def dp_pos_to_displacy_format(
    dp_result: List[Tuple[int, str, int, str]],
    pos_result: List[Tuple[str, str]] = None
) -&gt; List[Dict]:
    """Converts dependency parsing results to the spacy.displacy format."""
    if pos_result is None:
        pos_result = [(res[1], 'X') for res in dp_result]

    if len(dp_result) != len(pos_result) or any(d[1] != p[0] for d, p in zip(dp_result, pos_result)):
        logger.warning("DP and POS token mismatch. Displacy output may be incorrect.")
        words_data = [{'text': res[1], 'tag': 'X'} for res in dp_result]
    else:
        words_data = [{'text': token, 'tag': pos_tag} for (token, pos_tag) in pos_result]

    arcs_data = []
    for word_idx, _, head_idx, dep_label in dp_result:
        if head_idx &lt;= 0:
            continue
        start, end = word_idx - 1, head_idx - 1
        direction = 'left' if start &gt; end else 'right'
        if direction == 'left':
            start, end = end, start
        arcs_data.append({'start': start, 'end': end, 'label': dep_label, 'dir': direction})
    
    return [{'words': words_data, 'arcs': arcs_data}]
  </file>

  <file path="./vnlp_colab/dep/dep_treestack_utils_colab.py">
# coding=utf-8
#
# Copyright 2025 VNLP Project Authors.
#
# Licensed under the GNU Affero General Public License, Version 3.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.gnu.org/licenses/agpl-3.0.html
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Keras 3 compliant utilities for the TreeStack Dependency Parser (DP).

This module provides the modernized model creation and data processing functions
for the TreeStack DP model, ensuring compatibility with Keras 3 and Colab.
"""
import logging
from typing import List, Tuple

import numpy as np
import tensorflow as tf
from tensorflow import keras

logger = logging.getLogger(__name__)


def create_treestack_dp_model(
    word_embedding_vocab_size: int,
    word_embedding_vector_size: int,
    word_embedding_matrix: np.ndarray,
    pos_vocab_size: int,
    pos_embedding_vector_size: int,
    sentence_max_len: int,
    tag_max_len: int,
    arc_label_vector_len: int,
    num_rnn_stacks: int,
    tag_num_rnn_units: int,
    lc_num_rnn_units: int,
    lc_arc_label_num_rnn_units: int,
    rc_num_rnn_units: int,
    dropout: float,
    tag_embedding_matrix: np.ndarray,
    fc_units_multipliers: Tuple[int, int],
) -&gt; keras.Model:
    """
    Builds the TreeStack Dependency Parser model using the Keras 3 Functional API.

    This architecture is a 1:1 replica of the original model, ensuring
    weight compatibility while adhering to modern Keras standards.
    """
    logger.info("Creating Keras 3 TreeStack Dependency Parser model...")

    tag_vocab_size, tag_embed_size = tag_embedding_matrix.shape

    # --- 1. Define Shared Layers &amp; Sub-Models ---
    word_embedding_layer = keras.layers.Embedding(
        input_dim=word_embedding_vocab_size,
        output_dim=word_embedding_vector_size,
        embeddings_initializer=keras.initializers.Constant(word_embedding_matrix),
        trainable=False,
        name="word_embedding",
    )
    pos_embedding_layer = keras.layers.Embedding(
        input_dim=pos_vocab_size + 1,
        output_dim=pos_embedding_vector_size,
        name="pos_embedding",
    )
    tag_embedding_layer = keras.layers.Embedding(
        input_dim=tag_vocab_size,
        output_dim=tag_embed_size,
        weights=[tag_embedding_matrix],
        trainable=False,
        name="tag_embedding",
    )

    tag_rnn = keras.Sequential([
        keras.layers.GRU(tag_num_rnn_units, return_sequences=True),
        keras.layers.Dropout(dropout),
        keras.layers.GRU(tag_num_rnn_units),
        keras.layers.Dropout(dropout),
    ], name="tag_rnn")

    lc_rnn = keras.Sequential([
        keras.layers.GRU(lc_num_rnn_units, return_sequences=True),
        keras.layers.Dropout(dropout),
        keras.layers.GRU(lc_num_rnn_units),
        keras.layers.Dropout(dropout),
    ], name="left_context_rnn")

    lc_arc_label_rnn = keras.Sequential([
        keras.layers.GRU(lc_arc_label_num_rnn_units, return_sequences=True),
        keras.layers.Dropout(dropout),
        keras.layers.GRU(lc_arc_label_num_rnn_units),
        keras.layers.Dropout(dropout),
    ], name="lc_arc_label_rnn")

    rc_rnn = keras.Sequential([
        keras.layers.GRU(rc_num_rnn_units, return_sequences=True, go_backwards=True),
        keras.layers.Dropout(dropout),
        keras.layers.GRU(rc_num_rnn_units, go_backwards=True),
        keras.layers.Dropout(dropout),
    ], name="right_context_rnn")

    # --- 2. Define Functional API Inputs ---
    word_input = keras.Input(shape=(1,), name="word_input", dtype="int32")
    tag_input = keras.Input(shape=(tag_max_len,), name="tag_input", dtype="int32")
    pos_input = keras.Input(shape=(1,), name="pos_input", dtype="int32")
    lc_word_input = keras.Input(shape=(sentence_max_len,), name="lc_word_input", dtype="int32")
    lc_tag_input = keras.Input(shape=(sentence_max_len, tag_max_len), name="lc_tag_input", dtype="int32")
    lc_pos_input = keras.Input(shape=(sentence_max_len,), name="lc_pos_input", dtype="int32")
    lc_arc_label_input = keras.Input(shape=(sentence_max_len, arc_label_vector_len), name="lc_arc_label_input", dtype="float32")
    rc_word_input = keras.Input(shape=(sentence_max_len,), name="rc_word_input", dtype="int32")
    rc_tag_input = keras.Input(shape=(sentence_max_len, tag_max_len), name="rc_tag_input", dtype="int32")
    rc_pos_input = keras.Input(shape=(sentence_max_len,), name="rc_pos_input", dtype="int32")

    # --- 3. Build the Graph ---
    # Current word branch
    word_embedded = keras.layers.Flatten()(word_embedding_layer(word_input))
    tag_embedded = tag_embedding_layer(tag_input)
    tag_rnn_output = tag_rnn(tag_embedded)
    pos_embedded = keras.layers.Flatten()(pos_embedding_layer(pos_input))
    current_word_vector = keras.layers.Concatenate()([word_embedded, tag_rnn_output, pos_embedded])

    # Left context branch
    lc_word_embedded = word_embedding_layer(lc_word_input)
    lc_tag_embedded = tag_embedding_layer(lc_tag_input)
    lc_td_tag_rnn_output = keras.layers.TimeDistributed(tag_rnn)(lc_tag_embedded)
    lc_pos_embedded = pos_embedding_layer(lc_pos_input)
    lc_combined = keras.layers.Concatenate()([lc_word_embedded, lc_td_tag_rnn_output, lc_pos_embedded])
    lc_output = lc_rnn(lc_combined)
    lc_arc_label_output = lc_arc_label_rnn(lc_arc_label_input)

    # Right context branch
    rc_word_embedded = word_embedding_layer(rc_word_input)
    rc_tag_embedded = tag_embedding_layer(rc_tag_input)
    rc_td_tag_rnn_output = keras.layers.TimeDistributed(tag_rnn)(rc_tag_embedded)
    rc_pos_embedded = pos_embedding_layer(rc_pos_input)
    rc_combined = keras.layers.Concatenate()([rc_word_embedded, rc_td_tag_rnn_output, rc_pos_embedded])
    rc_output = rc_rnn(rc_combined)

    # --- 4. Concatenate and Final FC Layers ---
    concatenated = keras.layers.Concatenate()([current_word_vector, lc_output, lc_arc_label_output, rc_output])
    x = keras.layers.Dense(tag_num_rnn_units * fc_units_multipliers[0], activation="relu")(concatenated)
    x = keras.layers.Dropout(dropout)(x)
    x = keras.layers.Dense(tag_num_rnn_units * fc_units_multipliers[1], activation="relu")(x)
    x = keras.layers.Dropout(dropout)(x)
    arc_label_output = keras.layers.Dense(arc_label_vector_len, activation="sigmoid")(x)

    model = keras.Model(
        inputs=[
            word_input, tag_input, pos_input,
            lc_word_input, lc_tag_input, lc_pos_input, lc_arc_label_input,
            rc_word_input, rc_tag_input, rc_pos_input
        ],
        outputs=arc_label_output,
        name="TreeStack_DP_Model",
    )
    logger.info("TreeStack Dependency Parser model created successfully.")
    return model


def process_treestack_dp_input(
    tokens: List[str],
    sentence_analyses: List[str],
    pos_tags: List[str],
    previous_arcs: List[int],
    previous_labels: List[int],
    tokenizer_word: tf.keras.preprocessing.text.Tokenizer,
    tokenizer_morph_tag: tf.keras.preprocessing.text.Tokenizer,
    tokenizer_pos: tf.keras.preprocessing.text.Tokenizer,
    tokenizer_label: tf.keras.preprocessing.text.Tokenizer,
    word_form: str,
    sentence_max_len: int,
    tag_max_len: int,
    arc_label_vector_len: int,
) -&gt; Tuple[np.ndarray, ...]:
    """
    Prepares a batch of inputs for a single token for the TreeStack DP model.
    """
    stems = [analysis.split("+")[0] for analysis in sentence_analyses]
    label_vocab_size = len(tokenizer_label.word_index)
    t = len(previous_arcs)  # Current token index

    words_to_tokenize = []
    if word_form == 'whole':
        for whole, stem in zip(tokens, stems):
            if not tokenizer_word.texts_to_sequences([whole])[0] and tokenizer_word.texts_to_sequences([stem])[0]:
                words_to_tokenize.append(stem)
            else:
                words_to_tokenize.append(whole)
    else: # 'stem'
        for whole, stem in zip(tokens, stems):
            if not tokenizer_word.texts_to_sequences([stem])[0] and tokenizer_word.texts_to_sequences([whole])[0]:
                words_to_tokenize.append(whole)
            else:
                words_to_tokenize.append(stem)

    # --- Prepare inputs for the current token (t) ---
    word_t = tokenizer_word.texts_to_sequences([[words_to_tokenize[t]]])[0]
    tags_t_str = " ".join(sentence_analyses[t].split("+")[1:])
    tags_t = tokenizer_morph_tag.texts_to_sequences([tags_t_str])
    tags_t = keras.preprocessing.sequence.pad_sequences(tags_t, maxlen=tag_max_len, padding="pre")[0]
    pos_t = tokenizer_pos.texts_to_sequences([[pos_tags[t]]])[0]

    # --- Left Context ---
    lc_words = tokenizer_word.texts_to_sequences([words_to_tokenize[:t]])[0]
    lc_words = keras.preprocessing.sequence.pad_sequences([lc_words], maxlen=sentence_max_len, padding="pre")[0]
    
    lc_tags_str = [" ".join(a.split("+")[1:]) for a in sentence_analyses[:t]]
    lc_tags_seq = tokenizer_morph_tag.texts_to_sequences(lc_tags_str)
    lc_tags = keras.preprocessing.sequence.pad_sequences(lc_tags_seq, maxlen=tag_max_len, padding="pre")
    lc_tags_padded = np.zeros((sentence_max_len, tag_max_len), dtype=np.int32)
    if lc_tags.shape[0] &gt; 0:
        lc_tags_padded[-lc_tags.shape[0]:] = lc_tags

    lc_pos = tokenizer_pos.texts_to_sequences([pos_tags[:t]])[0]
    lc_pos = keras.preprocessing.sequence.pad_sequences([lc_pos], maxlen=sentence_max_len, padding="pre")[0]
    
    lc_arc_label_vectors = np.zeros((sentence_max_len, arc_label_vector_len), dtype=np.float32)
    if previous_arcs:
        for i, (arc, label) in enumerate(zip(previous_arcs, previous_labels)):
            arc_vec = tf.keras.utils.to_categorical(arc, num_classes=sentence_max_len + 1)
            label_vec = tf.keras.utils.to_categorical(label, num_classes=label_vocab_size + 1)
            combined = np.concatenate([arc_vec, label_vec])
            lc_arc_label_vectors[-(t - i)] = combined

    # --- Right Context ---
    rc_words = tokenizer_word.texts_to_sequences([words_to_tokenize[t + 1:]])[0]
    rc_words = keras.preprocessing.sequence.pad_sequences([rc_words], maxlen=sentence_max_len, padding="post", truncating="post")[0]
    
    rc_tags_str = [" ".join(a.split("+")[1:]) for a in sentence_analyses[t + 1:]]
    rc_tags_seq = tokenizer_morph_tag.texts_to_sequences(rc_tags_str)
    rc_tags = keras.preprocessing.sequence.pad_sequences(rc_tags_seq, maxlen=tag_max_len, padding="pre")
    rc_tags_padded = np.zeros((sentence_max_len, tag_max_len), dtype=np.int32)
    if rc_tags.shape[0] &gt; 0:
        rc_tags_padded[:rc_tags.shape[0]] = rc_tags
        
    rc_pos = tokenizer_pos.texts_to_sequences([pos_tags[t + 1:]])[0]
    rc_pos = keras.preprocessing.sequence.pad_sequences([rc_pos], maxlen=sentence_max_len, padding="post", truncating="post")[0]
    
    # --- Reshape for a single batch item ---
    return (
        np.array(word_t).reshape(1, 1), np.array(tags_t).reshape(1, tag_max_len), np.array(pos_t).reshape(1, 1),
        lc_words.reshape(1, sentence_max_len), lc_tags_padded.reshape(1, sentence_max_len, tag_max_len),
        lc_pos.reshape(1, sentence_max_len), lc_arc_label_vectors.reshape(1, sentence_max_len, arc_label_vector_len),
        rc_words.reshape(1, sentence_max_len), rc_tags_padded.reshape(1, sentence_max_len, tag_max_len),
        rc_pos.reshape(1, sentence_max_len)
    )
  </file>

  <file path="./vnlp_colab/dep/dep_colab.py">
# coding=utf-8
#
# Copyright 2025 VNLP Project Authors.
#
# Licensed under the GNU Affero General Public License, Version 3.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.gnu.org/licenses/agpl-3.0.html
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Dependency Parser (DP) module for VNLP Colab.

This module provides the high-level DependencyParser API and implementations
for SPUContextDP and the dependency-aware TreeStackDP, all refactored for
Keras 3 and high performance.
"""
import logging
import pickle
from typing import List, Tuple, Dict, Any, Union

import numpy as np
import sentencepiece as spm
import tensorflow as tf
from tensorflow import keras

# Updated imports for package structure
from vnlp_colab.utils_colab import download_resource, load_keras_tokenizer, get_vnlp_cache_dir
from vnlp_colab.dep.dep_utils_colab import (
    create_spucontext_dp_model, process_dp_input,
    decode_arc_label_vector, dp_pos_to_displacy_format
)
from vnlp_colab.dep.dep_treestack_utils_colab import (
    create_treestack_dp_model, process_treestack_dp_input
)
from vnlp_colab.stemmer.stemmer_colab import StemmerAnalyzer, get_stemmer_analyzer
from vnlp_colab.pos.pos_colab import PoSTagger

logger = logging.getLogger(__name__)

# --- Model &amp; Resource Configuration ---
_MODEL_CONFIGS = {
    'SPUContextDP': {
        'weights_prod': ("DP_SPUContext_prod.weights", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/DP_SPUContext_prod.weights"),
        'weights_eval': ("DP_SPUContext_eval.weights", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/DP_SPUContext_eval.weights"),
        'word_embedding_matrix': ("SPUTokenized_word_embedding_16k.matrix", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/SPUTokenized_word_embedding_16k.matrix"),
        'spu_tokenizer': ("SPU_word_tokenizer_16k.model", "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/vnlp/resources/SPU_word_tokenizer_16k.model"),
        'label_tokenizer': ("DP_label_tokenizer.json", "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/vnlp/dependency_parser/resources/DP_label_tokenizer.json"),
        'params': {'sentence_max_len': 40, 'word_embedding_dim': 128, 'num_rnn_stacks': 2, 'rnn_units_multiplier': 2, 'fc_units_multiplier': (2, 1), 'dropout': 0.2}
    },
    'TreeStackDP': {
        'weights_prod': ("DP_TreeStack_prod.weights", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/DP_TreeStack_prod.weights"),
        'weights_eval': ("DP_TreeStack_eval.weights", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/DP_TreeStack_eval.weights"),
        'word_embedding_matrix': ("TBWTokenized_word_embedding.matrix", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/TBWTokenized_word_embedding.matrix"),
        'word_tokenizer': ("TB_word_tokenizer.json", "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/vnlp/resources/TB_word_tokenizer.json"),
        'morph_tag_tokenizer': ("Stemmer_morph_tag_tokenizer.json", "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/vnlp/stemmer_morph_analyzer/resources/Stemmer_morph_tag_tokenizer.json"),
        'pos_label_tokenizer': ("PoS_label_tokenizer.json", "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/vnlp/part_of_speech_tagger/resources/PoS_label_tokenizer.json"),
        'dp_label_tokenizer': ("DP_label_tokenizer.json", "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/vnlp/dependency_parser/resources/DP_label_tokenizer.json"),
        'params': {'word_embedding_vector_size': 128, 'pos_embedding_vector_size': 8, 'num_rnn_stacks': 2, 'tag_num_rnn_units': 128, 'lc_num_rnn_units': 384, 'lc_arc_label_num_rnn_units': 384, 'rc_num_rnn_units': 384, 'fc_units_multipliers': (8, 4), 'word_form': 'whole', 'dropout': 0.2, 'sentence_max_len': 40, 'tag_max_len': 15}
    }
}

# --- Singleton Caching for Model Instances ---
_MODEL_INSTANCE_CACHE: Dict[str, Any] = {}


class SPUContextDP:
    """
    SentencePiece Unigram Context Dependency Parser.
    Optimized with tf.function for high-performance inference.
    """
    def __init__(self, evaluate: bool = False):
        """
        Initializes the model, loads weights, and compiles the prediction function.
        """
        logger.info(f"Initializing SPUContextDP model (evaluate={evaluate})...")
        config = _MODEL_CONFIGS['SPUContextDP']
        self.params = config['params']
        cache_dir = get_vnlp_cache_dir()

        # --- Download and Load Resources ---
        weights_file, weights_url = config['weights_eval'] if evaluate else config['weights_prod']
        weights_path = download_resource(weights_file, weights_url, cache_dir)
        embedding_path = download_resource(*config['word_embedding_matrix'], cache_dir)
        spu_tokenizer_path = download_resource(*config['spu_tokenizer'], cache_dir)
        label_tokenizer_path = download_resource(*config['label_tokenizer'], cache_dir)

        self.spu_tokenizer_word = spm.SentencePieceProcessor(model_file=str(spu_tokenizer_path))
        self.tokenizer_label = load_keras_tokenizer(label_tokenizer_path)
        self.label_vocab_size = len(self.tokenizer_label.word_index)
        self.arc_label_vector_len = self.params['sentence_max_len'] + 1 + self.label_vocab_size + 1
        word_embedding_matrix = np.load(embedding_path)

        # --- Build and Load Model ---
        num_rnn_units = self.params['word_embedding_dim'] * self.params['rnn_units_multiplier']
        self.model = create_spucontext_dp_model(
            vocab_size=self.spu_tokenizer_word.get_piece_size(),
            arc_label_vector_len=self.arc_label_vector_len,
            word_embedding_dim=self.params['word_embedding_dim'],
            word_embedding_matrix=np.zeros_like(word_embedding_matrix),
            num_rnn_units=num_rnn_units, **config['params']
        )
        with open(weights_path, 'rb') as fp: model_weights = pickle.load(fp)
        self.model.set_weights([word_embedding_matrix] + model_weights)
        self._initialize_compiled_predict_step()
        logger.info("SPUContextDP model initialized successfully.")

    def _initialize_compiled_predict_step(self):
        input_signature = [
            tf.TensorSpec(shape=(1, 8), dtype=tf.int32), # TOKEN_PIECE_MAX_LEN
            tf.TensorSpec(shape=(1, 40, 8), dtype=tf.int32), # SENTENCE_MAX_LEN
            tf.TensorSpec(shape=(1, 40, 8), dtype=tf.int32),
            tf.TensorSpec(shape=(1, 40, self.arc_label_vector_len), dtype=tf.float32),
        ]

        @tf.function(input_signature=input_signature)
        def predict_step(word, left_ctx, right_ctx, lc_arc_label_history):
            return self.model([word, left_ctx, right_ctx, lc_arc_label_history], training=False)
        self.compiled_predict_step = predict_step

    def predict(self, tokens: List[str]) -&gt; List[Tuple[int, str, int, str]]:
        """
        High-level API for Sentence Dependency Parsing.

        Args:
            tokens (list): Input sentence tokens.
        Returns:
            List[Tuple[int, str, int, str]]: A list of tuple of token_index, tokens, arc and DEP Parsing results 
        """
        if not tokens: return []
        arcs, labels = [], []
        for t in range(len(tokens)):
            inputs_np = process_dp_input(
                t, tokens, self.spu_tokenizer_word, self.tokenizer_label,
                self.arc_label_vector_len, arcs, labels
            )
            inputs_tf = [tf.convert_to_tensor(arr) for arr in inputs_np]
            logits = self.compiled_predict_step(*inputs_tf).numpy()[0]
            arc, label = decode_arc_label_vector(logits, self.params['sentence_max_len'], self.label_vocab_size)
            arcs.append(arc)
            labels.append(label)
        
        return [(i + 1, token, arcs[i], self.tokenizer_label.sequences_to_texts([[labels[i]]])[0] or "UNK")
                for i, token in enumerate(tokens)]

class TreeStackDP:
    """Implementation for the TreeStack Dependency Parser."""
    def __init__(self, stemmer_analyzer: StemmerAnalyzer, pos_tagger: PoSTagger, evaluate: bool = False):
        logger.info(f"Initializing TreeStackDP model (evaluate={evaluate})...")
        self.stemmer_analyzer = stemmer_analyzer
        self.pos_tagger = pos_tagger
        config = _MODEL_CONFIGS['TreeStackDP']
        self.params = config['params']
        cache_dir = get_vnlp_cache_dir()

        weights_file, weights_url = config['weights_eval'] if evaluate else config['weights_prod']
        weights_path = download_resource(weights_file, weights_url, cache_dir)
        embedding_path = download_resource(*config['word_embedding_matrix'], cache_dir)
        self.tokenizer_word = load_keras_tokenizer(download_resource(*config['word_tokenizer'], cache_dir))
        self.tokenizer_morph_tag = load_keras_tokenizer(download_resource(*config['morph_tag_tokenizer'], cache_dir))
        self.tokenizer_pos = load_keras_tokenizer(download_resource(*config['pos_label_tokenizer'], cache_dir))
        self.tokenizer_label = load_keras_tokenizer(download_resource(*config['dp_label_tokenizer'], cache_dir))
        
        word_embedding_matrix = np.load(embedding_path)
        tag_embedding_matrix = self.stemmer_analyzer.model.layers[5].weights[0].numpy()
        self.arc_label_vector_len = self.params['sentence_max_len'] + 1 + len(self.tokenizer_label.word_index) + 1
        
        self.model = create_treestack_dp_model(
            word_embedding_vocab_size=len(self.tokenizer_word.word_index) + 1,
            pos_vocab_size=len(self.tokenizer_pos.word_index),
            arc_label_vector_len=self.arc_label_vector_len,
            tag_embedding_matrix=tag_embedding_matrix,
            **self.params
        )
        with open(weights_path, 'rb') as fp: model_weights = pickle.load(fp)
        self.model.set_weights([model_weights[0], word_embedding_matrix] + model_weights[1:])
        logger.info("TreeStackDP model initialized successfully.")

    def predict(self, tokens: List[str]) -&gt; List[Tuple[int, str, int, str]]:
        if not tokens: return []
        
        sentence_analyses = self.stemmer_analyzer.predict(tokens)
        pos_results_tuples = self.pos_tagger.predict(tokens)
        pos_tags = [tag for _, tag in pos_results_tuples]
        
        arcs, labels = [], []
        for t in range(len(tokens)):
            x_inputs = process_treestack_dp_input(
                tokens, sentence_analyses, pos_tags, arcs, labels,
                self.tokenizer_word, self.tokenizer_morph_tag, self.tokenizer_pos, self.tokenizer_label,
                self.params['word_form'], self.params['sentence_max_len'], self.params['tag_max_len'],
                self.arc_label_vector_len
            )
            logits = self.model(x_inputs, training=False).numpy()[0]
            arc, label = decode_arc_label_vector(logits, self.params['sentence_max_len'], len(self.tokenizer_label.word_index))
            arcs.append(arc)
            labels.append(label)

        return [(i + 1, token, arcs[i], self.tokenizer_label.sequences_to_texts([[labels[i]]])[0] or "UNK")
                for i, token in enumerate(tokens)]

class DependencyParser:
    """
    Main API class for Dependency Parser implementations.
    Uses a singleton factory for efficient model instance management.
    """
    def __init__(self, model: str = 'SPUContextDP', evaluate: bool = False):
        self.available_models = ['SPUContextDP', 'TreeStackDP']
        if model not in self.available_models:
            raise ValueError(f"'{model}' is not a valid model. Try one of {self.available_models}")

        cache_key = f"dp_{model}_{'eval' if evaluate else 'prod'}"
        if cache_key not in _MODEL_INSTANCE_CACHE:
            logger.info(f"Instance for '{cache_key}' not found. Creating new one.")
            if model == 'SPUContextDP':
                instance = SPUContextDP(evaluate)
            elif model == 'TreeStackDP':
                stemmer = get_stemmer_analyzer(evaluate)
                # TreeStackDP requires a TreeStackPoS instance
                pos_tagger = PoSTagger(model='TreeStackPoS', evaluate=evaluate) 
                instance = TreeStackDP(stemmer, pos_tagger, evaluate)
            _MODEL_INSTANCE_CACHE[cache_key] = instance
        else:
            logger.info(f"Found cached instance for '{cache_key}'.")
        self.instance: Union[SPUContextDP, TreeStackDP] = _MODEL_INSTANCE_CACHE[cache_key]

    def predict(
        self, tokens: List[str], displacy_format: bool = False,
        pos_result: List[Tuple[str, str]] = None
    ) -&gt; List[Tuple[int, str, int, str]]:
        """High-level API for Dependency Parsing on pre-tokenized text."""
        # The displacy format requires the original sentence string, which we no longer pass.
        # We will reconstruct it for this specific feature.
        sentence_for_displacy = " ".join(tokens) if displacy_format else ""
        
        # The underlying predict methods now only need tokens.
        dp_result = self.instance.predict(tokens)
        
        if displacy_format:
            return dp_pos_to_displacy_format(dp_result, pos_result)
        return dp_result
  </file>

  <file path="./vnlp_colab/pipeline_colab.py">
# coding=utf-8
#
# Copyright 2025 VNLP Project Authors.
#
# Licensed under the GNU Affero General Public License, Version 3.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.gnu.org/licenses/agpl-3.0.html
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Unified NLP Pipeline for VNLP Colab.

This module provides a high-level API to run a sequence of NLP tasks on a
dataset, including PoS tagging, NER, dependency parsing, sentiment analysis,
and morphological analysis. It supports model selection and dependency chains.
"""
import logging
import re
from pathlib import Path
from typing import List, Dict, Any, Set

import pandas as pd
from tqdm.notebook import tqdm

# Updated imports for package structure
from vnlp_colab.utils_colab import setup_logging
from vnlp_colab.tokenizer_colab import TreebankWordTokenize
from vnlp_colab.normalizer.normalizer_colab import Normalizer
from vnlp_colab.pos.pos_colab import PoSTagger
from vnlp_colab.ner.ner_colab import NamedEntityRecognizer
from vnlp_colab.dep.dep_colab import DependencyParser
from vnlp_colab.stemmer.stemmer_colab import StemmerAnalyzer, get_stemmer_analyzer
from vnlp_colab.sentiment.sentiment_colab import SentimentAnalyzer

logger = logging.getLogger(__name__)
tqdm.pandas()


class VNLPipeline:
    """
    Orchestrates a full NLP analysis pipeline for a given dataset.
    """
    def __init__(self, models_to_load: List[str]):
        """
        Initializes the pipeline and loads the required models into memory.

        Args:
            models_to_load (List[str]): Models to init. Format: ['task'] or ['task:model_name'].
                Examples: ['pos', 'ner', 'dep:TreeStackDP', 'stemmer', 'sentiment']
        """
        setup_logging()
        logger.info("Initializing VNLP Pipeline...")
        self.models: Dict[str, Any] = {}
        
        # --- Dependency Resolution ---
        resolved_models: Set[str] = set(models_to_load)
        model_map: Dict[str, str] = {}

        for model_str in models_to_load:
            parts = model_str.split(':')
            task = parts[0]
            model_name = parts[1] if len(parts) &gt; 1 else None
            
            if task == 'dep' and model_name == 'TreeStackDP':
                resolved_models.add('pos:TreeStackPoS')
            if task == 'pos' and model_name == 'TreeStackPoS':
                resolved_models.add('stemmer')

        # Create a clean map for loading        
        for model_str in resolved_models:
            parts = model_str.split(':')
            task, model_name = (parts[0], parts[1]) if len(parts) &gt; 1 else (parts[0], None)
            model_map[task] = model_name

        logger.info(f"Resolved model loading order: {model_map}")

        # --- Model Loading with Dependency Injection ---
        if 'stemmer' in model_map:
            self.models['stemmer'] = get_stemmer_analyzer()
        
        if 'pos' in model_map:
            self.models['pos'] = PoSTagger(model=(model_map['pos'] or 'SPUContextPoS'))
        
        if 'dep' in model_map:
            self.models['dep'] = DependencyParser(model=(model_map['dep'] or 'SPUContextDP'))
            
        if 'ner' in model_map:
            self.models['ner'] = NamedEntityRecognizer(model=(model_map['ner'] or 'SPUContextNER'))

        if 'sentiment' in model_map:
            self.models['sentiment'] = SentimentAnalyzer()
        
        self.normalizer = Normalizer(stemmer_analyzer_instance=self.models.get('stemmer'))
        
        logger.info(f"Pipeline initialized successfully with models: {list(self.models.keys())}")

    def load_from_csv(self, file_path: str, pickle_path: str) -&gt; pd.DataFrame:
        logger.info(f"Loading data from '{file_path}'...")
        df = pd.read_csv(
            file_path, sep='\t', header=None,
            names=['t_code', 'ch_no', 'p_no', 's_no', 'sentence'],
            dtype={'sentence': 'string'}
        )
        logger.info(f"Loaded {len(df)} records. Saving initial pickle to '{pickle_path}'...")
        df.to_pickle(pickle_path)
        return df

    def run_preprocessing(self, df: pd.DataFrame) -&gt; pd.DataFrame:
        logger.info("Starting preprocessing...")
        df['sentence'] = df['sentence'].progress_apply(
            lambda s: re.sub(r'\s+', ' ', s).strip() if isinstance(s, str) else ""
        )
        logger.info("Step 1/4: Cleaned 'sentence' column.")
        df['no_accents'] = df['sentence'].progress_apply(self.normalizer.remove_accent_marks)
        logger.info("Step 2/4: Created 'no_accents' column.")
        df['tokens'] = df['no_accents'].progress_apply(TreebankWordTokenize)
        logger.info("Step 3/4: Created 'tokens' column.")
        df['tokens_40'] = df['tokens'].progress_apply(
            lambda tokens: [tokens[i:i + 40] for i in range(0, len(tokens), 40)] if tokens else []
        )
        logger.info("Step 4/4: Created 'tokens_40' column for Dependency Parser.")
        return df

    def run_analysis(self, df: pd.DataFrame) -&gt; pd.DataFrame:
        logger.info("Starting NLP model analysis in dependency order...")

        # --- Models are run in a fixed order to satisfy dependencies ---
        if 'sentiment' in self.models:
            logger.info("Running Sentiment Analysis...")
            df['sentiment'] = df['no_accents'].progress_apply(self.models['sentiment'].predict_proba)

        if 'stemmer' in self.models:
            logger.info("Running Morphological Analysis...")
            df['morph'] = df['tokens'].progress_apply(self.models['stemmer'].predict)
            logger.info("Deriving Lemmas...")
            df['lemma'] = df['morph'].progress_apply(
                lambda morph_list: [m.split("+")[0] for m in morph_list if '+' in m]
            )

        if 'pos' in self.models:
            logger.info(f"Running Part-of-Speech Tagging using {self.models['pos'].instance.__class__.__name__}...")
            df['pos_tuples'] = df['tokens'].progress_apply(self.models['pos'].predict)
            df['pos'] = df['pos_tuples'].progress_apply(lambda tuples: [tag for _, tag in tuples])
        
        if 'ner' in self.models:
            logger.info(f"Running Named Entity Recognition using {self.models['ner'].instance.__class__.__name__}...")
            df['ner'] = df.progress_apply(
                lambda row: [tag for _, tag in self.models['ner'].predict(row['no_accents'], row['tokens'])],
                axis=1
            )
        
        if 'dep' in self.models:
            logger.info(f"Running Dependency Parsing using {self.models['dep'].instance.__class__.__name__}...")
            def parse_sentence_batches(row):
                full_result = []
                pos_tuples = row.get('pos_tuples', [])
                if not pos_tuples:
                    pos_tuples = [(token, "X") for token in row['tokens']]

                token_counter = 0
                for batch_tokens in row['tokens_40']:
                    if not batch_tokens: continue
                    batch_pos_tuples = pos_tuples[token_counter : token_counter + len(batch_tokens)]
                    token_counter += len(batch_tokens)
                    batch_dp_result = self.models['dep'].predict(tokens=batch_tokens, pos_result=batch_pos_tuples)
                    simplified_batch = [(head, label) for _, _, head, label in batch_dp_result]
                    full_result.extend(simplified_batch)
                return full_result
            
            df['dep'] = df.progress_apply(parse_sentence_batches, axis=1)

        # Clean up intermediate columns        
        if 'pos_tuples' in df.columns:
            df = df.drop(columns=['pos_tuples'])

        logger.info("NLP model analysis complete.")
        return df

    def run(self, csv_path: str, output_pickle_path: str) -&gt; pd.DataFrame:
        """Executes the full pipeline: load, preprocess, analyze, and save."""
        df_initial = self.load_from_csv(csv_path, f"{Path(output_pickle_path).stem}.initial.pkl")
        df_preprocessed = self.run_preprocessing(df_initial)
        df_final = self.run_analysis(df_preprocessed)
        
        logger.info(f"Saving final processed DataFrame to '{output_pickle_path}'...")
        df_final.to_pickle(output_pickle_path)
        logger.info("Pipeline execution finished successfully.")
        return df_final

if __name__ == "__main__":
    logger.info("--- VNLP Colab Pipeline Standalone Test ---")
    
    dummy_data = (
        "novel01\t1\t1\t1\tBu film harikaydı, çok beğendim.\n"
        "novel01\t1\t1\t2\tBenim adım Melikşah ve İstanbul'da yaşıyorum.\n"
        "novel01\t1\t2\t1\tZamanımı boşa harcadığımı düşünüyorum.\n"
    )
    csv_path = Path("/content/dummy_input.csv")
    csv_path.write_text(dummy_data, encoding='utf-8')
    logger.info(f"Created dummy data at '{csv_path}'")

    # --- Test 1: Full Pipeline (SPUContext Defaults) ---
    logger.info("\n--- Running Test 1: Full SPUContext Pipeline ---")
    spu_models = ['pos', 'ner', 'dep', 'stemmer', 'sentiment']
    spu_pipeline = VNLPipeline(models_to_load=spu_models)
    spu_output_path = "/content/spu_output.pkl"
    spu_df = spu_pipeline.run(csv_path=str(csv_path), output_pickle_path=spu_output_path)

    print("\n--- SPUContext Pipeline Final DataFrame ---")
    pd.set_option('display.max_columns', None); pd.set_option('display.expand_frame_repr', False)
    print(spu_df.head())

    # --- Test 2: TreeStack Dependency Pipeline ---
    logger.info("\n--- Running Test 2: TreeStack Pipeline ---")
    treestack_models = ['dep:TreeStackDP', 'ner', 'sentiment']
    tree_pipeline = VNLPipeline(models_to_load=treestack_models)
    tree_output_path = "/content/treestack_output.pkl"
    tree_df = tree_pipeline.run(csv_path=str(csv_path), output_pickle_path=tree_output_path)
    
    print("\n--- TreeStack Pipeline Final DataFrame ---")
    print(tree_df.head())
  </file>

  <file path="./vnlp_colab/pos/pos_utils_colab.py">
# coding=utf-8
#
# Copyright 2025 VNLP Project Authors.
#
# Licensed under the GNU Affero General Public License, Version 3.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.gnu.org/licenses/agpl-3.0.html
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Keras 3 compliant utilities for the SPUContext Part-of-Speech (PoS) Tagger.

This module provides the modernized model creation function for the PoS tagger,
ensuring compatibility with the latest TensorFlow/Keras versions and Colab.
"""
import logging
from typing import List, Tuple

import numpy as np
import tensorflow as tf
from tensorflow import keras

# Updated import for package structure
from vnlp_colab.utils_colab import create_rnn_stacks, process_word_context

logger = logging.getLogger(__name__)

# --- Model Hyperparameters (as constants for clarity) ---
TOKEN_PIECE_MAX_LEN: int = 8
SENTENCE_MAX_LEN: int = 40

def create_spucontext_pos_model(
    vocab_size: int,
    pos_vocab_size: int,
    word_embedding_dim: int,
    word_embedding_matrix: np.ndarray,
    num_rnn_units: int,
    num_rnn_stacks: int,
    fc_units_multiplier: tuple[int, int],
    dropout: float
) -&gt; keras.Model:
    """
    Builds the SPUContext PoS tagging model using the Keras 3 Functional API.

    This architecture is a 1:1 replica of the original model blueprint, ensuring
    weight compatibility while adhering to modern Keras standards.

    Args:
        vocab_size (int): Vocabulary size for the word embedding layer.
        pos_vocab_size (int): Vocabulary size for the PoS tags. The output layer
            will have `pos_vocab_size + 1` units for the padding token.
        word_embedding_dim (int): Dimension of the word embeddings.
        word_embedding_matrix (np.ndarray): Pre-trained word embedding matrix.
        num_rnn_units (int): Number of units in the GRU layers.
        num_rnn_stacks (int): Number of layers in each RNN stack.
        fc_units_multiplier (tuple[int, int]): Multipliers for the dense layers.
        dropout (float): Dropout rate.

    Returns:
        keras.Model: The compiled Keras model.
    """
    logger.info("Creating Keras 3 SPUContext PoS Tagger model...")

    # --- 1. Define Functional API Inputs ---
    word_input = keras.Input(
        shape=(TOKEN_PIECE_MAX_LEN,), name='word_input', dtype='int32'
    )
    left_context_input = keras.Input(
        shape=(SENTENCE_MAX_LEN, TOKEN_PIECE_MAX_LEN), name='left_input', dtype='int32'
    )
    right_context_input = keras.Input(
        shape=(SENTENCE_MAX_LEN, TOKEN_PIECE_MAX_LEN), name='right_input', dtype='int32'
    )
    # Input for previously predicted PoS tags (one-hot encoded)
    lc_pos_input = keras.Input(
        shape=(SENTENCE_MAX_LEN, pos_vocab_size + 1), name='lc_pos_input', dtype='float32'
    )

    # --- 2. Define Reusable Sub-Models ---

    # Shared model for processing token pieces of a single word
    word_embedding_layer = keras.layers.Embedding(
        vocab_size,
        word_embedding_dim,
        embeddings_initializer=keras.initializers.Constant(word_embedding_matrix),
        trainable=False,
        name='WORD_EMBEDDING'
    )
    word_rnn_stack = create_rnn_stacks(num_rnn_stacks, num_rnn_units, dropout)
    word_rnn_model = keras.Sequential(
        [word_embedding_layer, word_rnn_stack], name="WORD_RNN"
    )

    # --- 3. Build the Four Parallel Branches of the Main Graph ---

    # Branch 1: Current word processing
    word_output = word_rnn_model(word_input)

    # Branch 2: Left context processing
    left_context_vectors = keras.layers.TimeDistributed(word_rnn_model)(left_context_input)
    left_context_stack = create_rnn_stacks(num_rnn_stacks, num_rnn_units, dropout)
    left_context_output = left_context_stack(left_context_vectors)

    # Branch 3: Right context processing (processes sequence in reverse)
    right_context_vectors = keras.layers.TimeDistributed(word_rnn_model)(right_context_input)
    right_context_stack = create_rnn_stacks(num_rnn_stacks, num_rnn_units, dropout, go_backwards=True)
    right_context_output = right_context_stack(right_context_vectors)

    # Branch 4: Previously predicted (left) PoS tags processing
    lc_pos_stack = create_rnn_stacks(num_rnn_stacks, num_rnn_units, dropout)
    lc_pos_output = lc_pos_stack(lc_pos_input)

    # --- 4. Concatenate Branches and Add Final FC Layers ---
    concatenated = keras.layers.Concatenate()(
        [word_output, left_context_output, right_context_output, lc_pos_output]
    )

    x = keras.layers.Dense(num_rnn_units * fc_units_multiplier[0], activation='relu')(concatenated)
    x = keras.layers.Dropout(dropout)(x)
    x = keras.layers.Dense(num_rnn_units * fc_units_multiplier[1], activation='relu')(x)
    x = keras.layers.Dropout(dropout)(x)
    pos_output = keras.layers.Dense(pos_vocab_size + 1, activation='softmax')(x)

    # --- 5. Build and Return the Final Model ---
    pos_model = keras.Model(
        inputs=[word_input, left_context_input, right_context_input, lc_pos_input],
        outputs=pos_output,
        name='SPUContext_PoS_Model'
    )
    logger.info("SPUContext PoS Tagger model created successfully.")
    return pos_model


def process_pos_input(
    word_index: int,
    sentence_tokens: List[str],
    spu_tokenizer_word: 'spm.SentencePieceProcessor',
    pos_label_tokenizer: tf.keras.preprocessing.text.Tokenizer,
    previous_predictions: List[int]
) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
    """
    Prepares input arrays for a single word for the PoS tagger model.

    This function vectorizes the word and its context, as well as the history of
    previously predicted PoS tags for the autoregressive loop.

    Args:
        word_index (int): Index of the current word in the sentence.
        sentence_tokens (List[str]): List of all tokens in the sentence.
        spu_tokenizer_word: The SentencePiece tokenizer.
        pos_label_tokenizer: The Keras tokenizer for PoS labels.
        previous_predictions (List[int]): A list of integer PoS tag predictions
            for the preceding words in the sentence.

    Returns:
        A tuple of NumPy arrays ready for model input:
        (current_word, left_context, right_context, left_pos_history)
    """
    pos_vocab_size = len(pos_label_tokenizer.word_index) + 1

    # 1. Process word and its context using the utility function
    current_word, left_context, right_context = process_word_context(
        word_index,
        sentence_tokens,
        spu_tokenizer_word,
        SENTENCE_MAX_LEN,
        TOKEN_PIECE_MAX_LEN
    )

    # 2. Create the history of previous PoS tag predictions
    left_pos_history = np.zeros((SENTENCE_MAX_LEN, pos_vocab_size), dtype=np.float32)

    if word_index &gt; 0:
        # Create one-hot vectors for all previous predictions
        one_hot_preds = tf.keras.utils.to_categorical(
            previous_predictions, num_classes=pos_vocab_size
        )
        # Place them in the correct time-steps of the input array
        # The history starts from the `SENTENCE_MAX_LEN - word_index`-th position
        start_pos = SENTENCE_MAX_LEN - word_index
        end_pos = start_pos + len(one_hot_preds)
        left_pos_history[start_pos:end_pos] = one_hot_preds

    # 3. Expand dims to create a batch of 1 for model prediction
    return (
        np.expand_dims(current_word, axis=0),
        np.expand_dims(left_context, axis=0),
        np.expand_dims(right_context, axis=0),
        np.expand_dims(left_pos_history, axis=0)
    )
  </file>

  <file path="./vnlp_colab/pos/pos_colab.py">
# coding=utf-8
#
# Copyright 2025 VNLP Project Authors.
#
# Licensed under the GNU Affero General Public License, Version 3.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.gnu.org/licenses/agpl-3.0.html
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Part-of-Speech (PoS) Tagger module for VNLP Colab.

This module contains the high-level PoSTagger API, which uses a singleton
factory to efficiently manage and serve different PoS tagging models, including
SPUContextPoS and the dependency-aware TreeStackPoS.
"""
import logging
import pickle
from typing import List, Tuple, Dict, Any, Union

import numpy as np
import sentencepiece as spm
import tensorflow as tf
from tensorflow import keras

# Updated imports for package structure
from vnlp_colab.utils_colab import download_resource, load_keras_tokenizer, get_vnlp_cache_dir
from vnlp_colab.pos.pos_utils_colab import create_spucontext_pos_model, process_pos_input
from vnlp_colab.pos.pos_treestack_utils_colab import create_treestack_pos_model, process_treestack_pos_input
from vnlp_colab.stemmer.stemmer_colab import StemmerAnalyzer, get_stemmer_analyzer

logger = logging.getLogger(__name__)

# --- Model &amp; Resource Configuration ---
_MODEL_CONFIGS = {
    'SPUContextPoS': {
        'weights_prod': ("PoS_SPUContext_prod.weights", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/PoS_SPUContext_prod.weights"),
        'weights_eval': ("PoS_SPUContext_eval.weights", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/PoS_SPUContext_eval.weights"),
        'word_embedding_matrix': ("SPUTokenized_word_embedding_16k.matrix", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/SPUTokenized_word_embedding_16k.matrix"),
        'spu_tokenizer': ("SPU_word_tokenizer_16k.model", "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/vnlp/resources/SPU_word_tokenizer_16k.model"),
        'label_tokenizer': ("PoS_label_tokenizer.json", "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/vnlp/part_of_speech_tagger/resources/PoS_label_tokenizer.json"),
        'params': { 'word_embedding_dim': 128, 'num_rnn_stacks': 1, 'rnn_units_multiplier': 1, 'fc_units_multiplier': (2, 1), 'dropout': 0.2 }
    },
    'TreeStackPoS': {
        'weights_prod': ("PoS_TreeStack_prod.weights", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/PoS_TreeStack_prod.weights"),
        'weights_eval': ("PoS_TreeStack_eval.weights", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/PoS_TreeStack_eval.weights"),
        'word_embedding_matrix': ("TBWTokenized_word_embedding.matrix", "https://vnlp-model-weights.s3.eu-west-1.amazonaws.com/TBWTokenized_word_embedding.matrix"),
        'word_tokenizer': ("TB_word_tokenizer.json", "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/vnlp/resources/TB_word_tokenizer.json"),
        'morph_tag_tokenizer': ("Stemmer_morph_tag_tokenizer.json", "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/vnlp/stemmer_morph_analyzer/resources/Stemmer_morph_tag_tokenizer.json"),
        'pos_label_tokenizer': ("PoS_label_tokenizer.json", "https://raw.githubusercontent.com/vngrs-ai/vnlp/main/vnlp/part_of_speech_tagger/resources/PoS_label_tokenizer.json"),
        'params': { 'word_embedding_vector_size': 128, 'num_rnn_stacks': 2, 'tag_num_rnn_units': 128, 'lc_num_rnn_units': 256, 'rc_num_rnn_units': 256, 'fc_units_multipliers': (2, 1), 'word_form': 'whole', 'dropout': 0.2, 'sentence_max_len': 40, 'tag_max_len': 15 }
    }
}

# --- Singleton Caching for Model Instances ---
_MODEL_INSTANCE_CACHE: Dict[str, Any] = {}


class SPUContextPoS:
    """
    SentencePiece Unigram Context Part-of-Speech Tagger.

    Optimized with tf.function for high-performance inference. It uses an
    autoregressive mechanism, where the prediction for each token is conditioned
    on the predictions of previous tokens.
    """
    def __init__(self, evaluate: bool = False):
        """
        Initializes the model, loads weights, and compiles the prediction function.
        This is a heavyweight operation managed by the singleton factory.
        """     
        logger.info(f"Initializing SPUContextPoS model (evaluate={evaluate})...")
        config = _MODEL_CONFIGS['SPUContextPoS']
        cache_dir = get_vnlp_cache_dir()

        # --- Download and Load Resources ---
        weights_file, weights_url = config['weights_eval'] if evaluate else config['weights_prod']
        weights_path = download_resource(weights_file, weights_url, cache_dir)
        embedding_matrix_path = download_resource(*config['word_embedding_matrix'], cache_dir)
        spu_tokenizer_path = download_resource(*config['spu_tokenizer'], cache_dir)
        label_tokenizer_path = download_resource(*config['label_tokenizer'], cache_dir)

        self.spu_tokenizer_word = spm.SentencePieceProcessor(model_file=str(spu_tokenizer_path))
        self.tokenizer_label = load_keras_tokenizer(label_tokenizer_path)
        self._label_index_word = {i: w for w, i in self.tokenizer_label.word_index.items()}
        word_embedding_matrix = np.load(embedding_matrix_path)

        # --- Build and Load Model ---
        params = config['params']
        num_rnn_units = params['word_embedding_dim'] * params['rnn_units_multiplier']
        
        self.model = create_spucontext_pos_model(
            vocab_size=self.spu_tokenizer_word.get_piece_size(),
            pos_vocab_size=len(self.tokenizer_label.word_index),
            word_embedding_dim=params['word_embedding_dim'],
            word_embedding_matrix=np.zeros_like(word_embedding_matrix),
            num_rnn_units=num_rnn_units,
            num_rnn_stacks=params['num_rnn_stacks'],
            fc_units_multiplier=params['fc_units_multiplier'],
            dropout=params['dropout']
        )

        with open(weights_path, 'rb') as fp:
            model_weights = pickle.load(fp)
        
        # The non-trainable embedding matrix is the first weight, followed by trainable weights.
        self.model.set_weights([word_embedding_matrix] + model_weights)
        self._initialize_compiled_predict_step()
        logger.info("SPUContextPoS model initialized successfully.")

    def _initialize_compiled_predict_step(self):
        """Creates a compiled TensorFlow function for the model's forward pass."""
        pos_vocab_size = len(self.tokenizer_label.word_index)
        input_signature = [
            tf.TensorSpec(shape=(1, 8), dtype=tf.int32), # TOKEN_PIECE_MAX_LEN = 8
            tf.TensorSpec(shape=(1, 40, 8), dtype=tf.int32), # SENTENCE_MAX_LEN = 40
            tf.TensorSpec(shape=(1, 40, 8), dtype=tf.int32),
            tf.TensorSpec(shape=(1, 40, pos_vocab_size + 1), dtype=tf.float32),
        ]

        @tf.function(input_signature=input_signature)
        def predict_step(word, left_ctx, right_ctx, lc_pos_history):
            return self.model([word, left_ctx, right_ctx, lc_pos_history], training=False)
        self.compiled_predict_step = predict_step

    def predict(self, tokens: List[str]) -&gt; List[Tuple[str, str]]:
        """
        Predicts PoS tags for a list of tokens using an optimized autoregressive loop.

        Args:
            tokens (List[str]): Input sentence tokens.

        Returns:
            List[Tuple[str, str]]: A list of (token, pos_label) tuples.
        """
        if not tokens:
            return []

        int_preds: List[int] = []

        for t in range(len(tokens)):
            # 1. Prepare inputs using the optimized helper function. 
            inputs_np = process_pos_input(t, tokens, self.spu_tokenizer_word, self.tokenizer_label, int_preds)
            # 2. Convert inputs to Tensors and call the compiled prediction function.
            inputs_tf = [tf.convert_to_tensor(arr) for arr in inputs_np]
            logits = self.compiled_predict_step(*inputs_tf).numpy()[0]
            # 3. Decode result and update state for the next iteration.
            int_pred = np.argmax(logits, axis=-1)
            int_preds.append(int_pred)
        # 4. Convert final integer predictions to text labels using a fast lookup
        pos_labels = [self._label_index_word.get(p, 'UNK') for p in int_preds]
        return list(zip(tokens, pos_labels))

class TreeStackPoS:
    """
    Tree-stack Part of Speech Tagger class.

    - This Part of Speech Tagger is *inspired* by `Tree-stack LSTM in Transition Based Dependency Parsing &lt;https://aclanthology.org/K18-2012/&gt;`_.
    - "Inspire" is emphasized because this implementation uses the approach of using Morphological Tags, Pre-trained word embeddings and POS tags as input for the model, rather than implementing the exact network proposed in the paper.
    - It achieves 0.89 Accuracy and 0.71 F1_macro_score on test sets of Universal Dependencies 2.9.
    - Input data is processed by NLTK.tokenize.TreebankWordTokenizer.
    - For more details about the training procedure, dataset and evaluation metrics, see `ReadMe &lt;https://github.com/vngrs-ai/VNLP/blob/main/vnlp/part_of_speech_tagger/ReadMe.md&gt;`_.
    """
    def __init__(self, stemmer_analyzer: StemmerAnalyzer, evaluate: bool = False):
        logger.info(f"Initializing TreeStackPoS model (evaluate={evaluate})...")
        self.stemmer_analyzer = stemmer_analyzer
        config = _MODEL_CONFIGS['TreeStackPoS']
        self.params = config['params']
        cache_dir = get_vnlp_cache_dir()
        # Check and download word embedding matrix and model weights
        weights_file, weights_url = config['weights_eval'] if evaluate else config['weights_prod']
        weights_path = download_resource(weights_file, weights_url, cache_dir)
        embedding_path = download_resource(*config['word_embedding_matrix'], cache_dir)
        self.tokenizer_word = load_keras_tokenizer(download_resource(*config['word_tokenizer'], cache_dir))
        self.tokenizer_morph_tag = load_keras_tokenizer(download_resource(*config['morph_tag_tokenizer'], cache_dir))
        self.tokenizer_pos_label = load_keras_tokenizer(download_resource(*config['pos_label_tokenizer'], cache_dir))
        self._pos_index_word = {i: w for w, i in self.tokenizer_pos_label.word_index.items()}
        # Load Word embedding matrix
        word_embedding_matrix = np.load(embedding_path)
        tag_embedding_matrix = self.stemmer_analyzer.model.layers[5].weights[0].numpy()
        # Load Model weights
        self.model = create_treestack_pos_model(
            word_embedding_vocab_size=len(self.tokenizer_word.word_index) + 1,
            pos_vocab_size=len(self.tokenizer_pos_label.word_index),
            tag_embedding_matrix=tag_embedding_matrix,
            **self.params
        )
        with open(weights_path, 'rb') as fp:
            model_weights = pickle.load(fp)
        
        self.model.set_weights([model_weights[0], word_embedding_matrix] + model_weights[1:])
        logger.info("TreeStackPoS model initialized successfully.")

    def predict(self, tokens: List[str]) -&gt; List[Tuple[str, str]]:
        """
        Args:
            sentence:
                Input text(sentence).

        Returns:
             List of (token, pos_label).
        """
        if not tokens:
            return []
            
        sentence_analyses = self.stemmer_analyzer.predict(tokens)
        
        pos_int_labels: List[int] = []
        for t in range(len(tokens)):
            x_inputs = process_treestack_pos_input(
                tokens, sentence_analyses, pos_int_labels, self.tokenizer_word,
                self.tokenizer_morph_tag, self.tokenizer_pos_label, self.params['word_form'],
                self.params['sentence_max_len'], self.params['tag_max_len']
            )
            raw_pred = self.model(x_inputs, training=False).numpy()[0]
            pos_int_label = np.argmax(raw_pred, axis=-1)
            pos_int_labels.append(pos_int_label)

        # Converting integer labels to text form 
        pos_labels = [self._pos_index_word.get(p, 'UNK') for p in pos_int_labels]
        return list(zip(tokens, pos_labels))


class PoSTagger:
    """
    Main API class for Part-of-Speech Tagger implementations.

    This class uses a singleton factory to ensure that heavy models are loaded
    into memory only once, making subsequent initializations instantaneous.

    Available models: ['SPUContextPoS']

    Example::
        from pos_colab import PoSTagger
        # First initialization is slow as it downloads and loads the model.
        pos_tagger = PoSTagger(model='SPUContextPoS')
        # Second initialization is instantaneous.
        pos_tagger2 = PoSTagger(model='SPUContextPoS')

        sentence = "Vapurla Beşiktaş'a geçip yürüyerek Maçka Parkı'na ulaştım."
        predictions = pos_tagger.predict(sentence)
        print(predictions)
        # Output: [('Vapurla', 'Noun'), ("Beşiktaş'a", 'Propn'), ('geçip', 'Verb'), ...]
    """
    def __init__(self, model: str = 'SPUContextPoS', evaluate: bool = False):
        self.available_models = ['SPUContextPoS', 'TreeStackPoS']
        if model not in self.available_models:
            raise ValueError(f"'{model}' is not a valid model. Try one of {self.available_models}")

        # Singleton factory logic
        cache_key = f"pos_{model}_{'eval' if evaluate else 'prod'}"
        if cache_key not in _MODEL_INSTANCE_CACHE:
            logger.info(f"Instance for '{cache_key}' not found. Creating new one.")
            if model == 'SPUContextPoS':
                instance = SPUContextPoS(evaluate)
            elif model == 'TreeStackPoS':
                stemmer_instance = get_stemmer_analyzer(evaluate)
                instance = TreeStackPoS(stemmer_instance, evaluate)
            _MODEL_INSTANCE_CACHE[cache_key] = instance
        else:
            logger.info(f"Found cached instance for '{cache_key}'.")

        self.instance: Union[SPUContextPoS, TreeStackPoS] = _MODEL_INSTANCE_CACHE[cache_key]

    def predict(self, tokens: List[str]) -&gt; List[Tuple[str, str]]:
        """Predicts PoS tags for a pre-tokenized list of words."""
        return self.instance.predict(tokens)


# --- Main Entry Point for Standalone Use ---
def main():
    """Demonstrates and tests the PoS Tagger module."""
    setup_logging()
    logger.info("--- VNLP Colab PoS Tagger Test Suite ---")
    sentence = "Vapurla Beşiktaş'a geçip yürüyerek Maçka Parkı'na ulaştım."
    tokens = TreebankWordTokenize(sentence)

    try:
        logger.info("\n1. Testing SPUContextPoS...")
        pos_spu = PoSTagger(model='SPUContextPoS')
        result_spu = pos_spu.predict(tokens)
        logger.info(f"   Input: {tokens}")
        logger.info(f"   SPUContextPoS Output: {result_spu}")
        assert len(result_spu) == len(tokens) and result_spu[1][1] == 'PROPN'
        logger.info("   SPUContextPoS test PASSED.")
    except Exception as e:
        logger.error(f"   SPUContextPoS test FAILED: {e}", exc_info=True)

    try:
        logger.info("\n2. Testing TreeStackPoS...")
        pos_tree = PoSTagger(model='TreeStackPoS')
        result_tree = pos_tree.predict(tokens)
        logger.info(f"   Input: {tokens}")
        logger.info(f"   TreeStackPoS Output: {result_tree}")
        assert len(result_tree) == len(tokens) and result_tree[1][1] == 'PROPN'
        logger.info("   TreeStackPoS test PASSED.")
    except Exception as e:
        logger.error(f"   TreeStackPoS test FAILED: {e}", exc_info=True)
    
    logger.info("\n3. Testing Singleton Caching...")
    import time
    start_time = time.time()
    _ = PoSTagger(model='SPUContextPoS')
    end_time = time.time()
    logger.info(f"   Re-initialization took: {end_time - start_time:.4f} seconds.")
    assert (end_time - start_time) &lt; 0.1
    logger.info("   Singleton Caching test PASSED.")

if __name__ == "__main__":
    main()
  </file>

  <file path="./vnlp_colab/pos/pos_treestack_utils_colab.py">
# coding=utf-8
#
# Copyright 2025 VNLP Project Authors.
#
# Licensed under the GNU Affero General Public License, Version 3.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     https://www.gnu.org/licenses/agpl-3.0.html
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
Keras 3 compliant utilities for the TreeStack Part-of-Speech (PoS) Tagger.

This module provides the modernized model creation and data processing functions
for the TreeStack PoS tagger, ensuring compatibility with Keras 3 and Colab.
"""
import logging
from typing import List, Tuple

import numpy as np
import tensorflow as tf
from tensorflow import keras


logger = logging.getLogger(__name__)


def create_treestack_pos_model(
    word_embedding_vocab_size: int,
    word_embedding_vector_size: int,
    word_embedding_matrix: np.ndarray,
    pos_vocab_size: int,
    sentence_max_len: int,
    tag_max_len: int,
    num_rnn_stacks: int,
    tag_num_rnn_units: int,
    lc_num_rnn_units: int,
    rc_num_rnn_units: int,
    dropout: float,
    tag_embedding_matrix: np.ndarray,
    fc_units_multipliers: Tuple[int, int],
) -&gt; keras.Model:
    """
    Builds the TreeStack PoS tagger model using the Keras 3 Functional API.

    This architecture is a 1:1 replica of the original model, ensuring
    weight compatibility while adhering to modern Keras standards.
    """
    logger.info("Creating Keras 3 TreeStack PoS Tagger model...")

    tag_vocab_size, tag_embed_size = tag_embedding_matrix.shape

    # --- 1. Define Shared Layers &amp; Sub-Models ---
    word_embedding_layer = keras.layers.Embedding(
        input_dim=word_embedding_vocab_size,
        output_dim=word_embedding_vector_size,
        embeddings_initializer=keras.initializers.Constant(word_embedding_matrix),
        trainable=False,
        name="word_embedding",
    )
    tag_embedding_layer = keras.layers.Embedding(
        input_dim=tag_vocab_size,
        output_dim=tag_embed_size,
        weights=[tag_embedding_matrix],
        trainable=False,
        name="tag_embedding",
    )

    tag_rnn = keras.Sequential(name="tag_rnn")
    for _ in range(num_rnn_stacks - 1):
        tag_rnn.add(keras.layers.GRU(tag_num_rnn_units, return_sequences=True))
        tag_rnn.add(keras.layers.Dropout(dropout))
    tag_rnn.add(keras.layers.GRU(tag_num_rnn_units))
    tag_rnn.add(keras.layers.Dropout(dropout))

    lc_rnn = keras.Sequential(name="left_context_rnn")
    for _ in range(num_rnn_stacks - 1):
        lc_rnn.add(keras.layers.GRU(lc_num_rnn_units, return_sequences=True))
        lc_rnn.add(keras.layers.Dropout(dropout))
    lc_rnn.add(keras.layers.GRU(lc_num_rnn_units))
    lc_rnn.add(keras.layers.Dropout(dropout))

    rc_rnn = keras.Sequential(name="right_context_rnn")
    for _ in range(num_rnn_stacks - 1):
        rc_rnn.add(keras.layers.GRU(rc_num_rnn_units, return_sequences=True, go_backwards=True))
        rc_rnn.add(keras.layers.Dropout(dropout))
    rc_rnn.add(keras.layers.GRU(rc_num_rnn_units, go_backwards=True))
    rc_rnn.add(keras.layers.Dropout(dropout))

    # --- 2. Define Functional API Inputs ---
    word_input = keras.Input(shape=(1,), name="word_input", dtype="int32")
    tag_input = keras.Input(shape=(tag_max_len,), name="tag_input", dtype="int32")
    lc_word_input = keras.Input(shape=(sentence_max_len,), name="lc_word_input", dtype="int32")
    lc_tag_input = keras.Input(shape=(sentence_max_len, tag_max_len), name="lc_tag_input", dtype="int32")
    lc_pos_input = keras.Input(shape=(sentence_max_len, pos_vocab_size + 1), name="lc_pos_input", dtype="float32")
    rc_word_input = keras.Input(shape=(sentence_max_len,), name="rc_word_input", dtype="int32")
    rc_tag_input = keras.Input(shape=(sentence_max_len, tag_max_len), name="rc_tag_input", dtype="int32")

    # --- 3. Build the Graph ---
    # Current word branch
    word_embedded = keras.layers.Flatten()(word_embedding_layer(word_input))
    tag_embedded = tag_embedding_layer(tag_input)
    tag_rnn_output = tag_rnn(tag_embedded)
    current_word_vector = keras.layers.Concatenate()([word_embedded, tag_rnn_output])

    # Left context branch
    lc_word_embedded = word_embedding_layer(lc_word_input)
    lc_tag_embedded = tag_embedding_layer(lc_tag_input)
    lc_td_tag_rnn_output = keras.layers.TimeDistributed(tag_rnn)(lc_tag_embedded)
    lc_combined = keras.layers.Concatenate()([lc_word_embedded, lc_td_tag_rnn_output, lc_pos_input])
    lc_output = lc_rnn(lc_combined)

    # Right context branch
    rc_word_embedded = word_embedding_layer(rc_word_input)
    rc_tag_embedded = tag_embedding_layer(rc_tag_input)
    rc_td_tag_rnn_output = keras.layers.TimeDistributed(tag_rnn)(rc_tag_embedded)
    rc_combined = keras.layers.Concatenate()([rc_word_embedded, rc_td_tag_rnn_output])
    rc_output = rc_rnn(rc_combined)

    # --- 4. Concatenate and Final FC Layers ---
    concatenated = keras.layers.Concatenate()([current_word_vector, lc_output, rc_output])
    x = keras.layers.Dense(tag_num_rnn_units * fc_units_multipliers[0], activation="relu")(concatenated)
    x = keras.layers.Dropout(dropout)(x)
    x = keras.layers.Dense(tag_num_rnn_units * fc_units_multipliers[1], activation="relu")(x)
    x = keras.layers.Dropout(dropout)(x)
    pos_output = keras.layers.Dense(pos_vocab_size + 1, activation="sigmoid")(x)

    model = keras.Model(
        inputs=[word_input, tag_input, lc_word_input, lc_tag_input, lc_pos_input, rc_word_input, rc_tag_input],
        outputs=pos_output,
        name="TreeStack_PoS_Model",
    )
    logger.info("TreeStack PoS Tagger model created successfully.")
    return model


def process_treestack_pos_input(
    tokens: List[str],
    sentence_analyses: List[str],
    previous_pos_preds: List[int],
    tokenizer_word: tf.keras.preprocessing.text.Tokenizer,
    tokenizer_morph_tag: tf.keras.preprocessing.text.Tokenizer,
    tokenizer_pos_label: tf.keras.preprocessing.text.Tokenizer,
    word_form: str,
    sentence_max_len: int,
    tag_max_len: int,
) -&gt; Tuple[np.ndarray, ...]:
    """
    Prepares a batch of inputs for a single token for the TreeStack PoS model.
    """
    stems = [analysis.split("+")[0] for analysis in sentence_analyses]
    pos_vocab_size = len(tokenizer_pos_label.word_index)
    pos_vector_size = pos_vocab_size + 1
    
    # Determine which word form to use (whole vs. stem)
    words_to_tokenize = []
    if word_form == 'whole':
        for whole, stem in zip(tokens, stems):
            if not tokenizer_word.texts_to_sequences([whole])[0] and tokenizer_word.texts_to_sequences([stem])[0]:
                words_to_tokenize.append(stem)
            else:
                words_to_tokenize.append(whole)
    else: # 'stem'
        for whole, stem in zip(tokens, stems):
            if not tokenizer_word.texts_to_sequences([stem])[0] and tokenizer_word.texts_to_sequences([whole])[0]:
                words_to_tokenize.append(whole)
            else:
                words_to_tokenize.append(stem)

    # --- Prepare inputs for the current token (t) ---
    t = len(previous_pos_preds)
    
    # 1. Current word, tag, and pos
    word_t = tokenizer_word.texts_to_sequences([[words_to_tokenize[t]]])[0]
    tags_t_str = " ".join(sentence_analyses[t].split("+")[1:])
    tags_t = tokenizer_morph_tag.texts_to_sequences([tags_t_str])
    tags_t = keras.preprocessing.sequence.pad_sequences(tags_t, maxlen=tag_max_len, padding="pre")[0]

    # 2. Left Context
    lc_words_str = words_to_tokenize[:t]
    lc_words = tokenizer_word.texts_to_sequences([lc_words_str])[0]
    lc_words = keras.preprocessing.sequence.pad_sequences([lc_words], maxlen=sentence_max_len, padding="pre")[0]
    
    lc_tags_str = [" ".join(a.split("+")[1:]) for a in sentence_analyses[:t]]
    lc_tags_seq = tokenizer_morph_tag.texts_to_sequences(lc_tags_str)
    lc_tags = keras.preprocessing.sequence.pad_sequences(lc_tags_seq, maxlen=tag_max_len, padding="pre")
    
    # Pad the sequence of tag vectors
    lc_tags_padded = np.zeros((sentence_max_len, tag_max_len), dtype=np.int32)
    if lc_tags.shape[0] &gt; 0:
        lc_tags_padded[-lc_tags.shape[0]:] = lc_tags

    lc_pos_vectors = np.zeros((sentence_max_len, pos_vector_size), dtype=np.float32)
    if previous_pos_preds:
        one_hot_pos = tf.keras.utils.to_categorical(previous_pos_preds, num_classes=pos_vector_size)
        lc_pos_vectors[-len(one_hot_pos):] = one_hot_pos

    # 3. Right Context
    rc_words_str = words_to_tokenize[t + 1:]
    rc_words = tokenizer_word.texts_to_sequences([rc_words_str])[0]
    rc_words = keras.preprocessing.sequence.pad_sequences([rc_words], maxlen=sentence_max_len, padding="post", truncating="post")[0]
    
    rc_tags_str = [" ".join(a.split("+")[1:]) for a in sentence_analyses[t + 1:]]
    rc_tags_seq = tokenizer_morph_tag.texts_to_sequences(rc_tags_str)
    rc_tags = keras.preprocessing.sequence.pad_sequences(rc_tags_seq, maxlen=tag_max_len, padding="pre")
    
    rc_tags_padded = np.zeros((sentence_max_len, tag_max_len), dtype=np.int32)
    if rc_tags.shape[0] &gt; 0:
        rc_tags_padded[:rc_tags.shape[0]] = rc_tags
    
    # 4. Reshape for a single batch item
    return (
        np.array(word_t).reshape(1, 1),
        tags_t.reshape(1, tag_max_len),
        lc_words.reshape(1, sentence_max_len),
        lc_tags_padded.reshape(1, sentence_max_len, tag_max_len),
        lc_pos_vectors.reshape(1, sentence_max_len, pos_vector_size),
        rc_words.reshape(1, sentence_max_len),
        rc_tags_padded.reshape(1, sentence_max_len, tag_max_len),
    )
  </file>

  <file path="./tests/test_stemmer.py">
# tests/test_stemmer.py
import pytest
from typing import List

# Assuming the package is installed or in the python path
from vnlp_colab.stemmer.stemmer_colab import get_stemmer_analyzer, StemmerAnalyzer
from vnlp_colab.utils_colab import setup_logging

# Initialize logging once for the test module
setup_logging()

@pytest.fixture(scope="module")
def sample_tokens_stemmer() -&gt; List[str]:
    """Provides a sample tokenized sentence for Stemmer/Morphological Analyzer tests."""
    return ["Üniversite", "sınavlarına", "canla", "başla", "çalışıyorlardı", "."]

def test_stemmer_analyzer_predict(sample_tokens_stemmer: List[str]):
    """
    Unit test for the StemmerAnalyzer model.
    Verifies output format, length, and the correctness of a few key analyses.
    """
    stemmer = get_stemmer_analyzer()
    result = stemmer.predict(sample_tokens_stemmer)

    assert isinstance(result, list)
    assert len(result) == len(sample_tokens_stemmer)
    assert all(isinstance(item, str) for item in result)

    # Validate specific, high-confidence morphological analyses
    # These represent the model's expected disambiguation choice.
    expected_analyses = {
        "Üniversite": "üniversite+Noun+A3sg+Pnon+Nom",
        "sınavlarına": "sınav+Noun+A3pl+P3sg+Dat",
        "canla": "can+Noun+A3sg+Pnon+Ins",
        "çalışıyorlardı": "çalış+Verb+Pos+Prog1+A3pl+Past",
        ".": ".+Punc",
    }

    result_dict = dict(zip(sample_tokens_stemmer, result))
    for token, expected_analysis in expected_analyses.items():
        assert result_dict.get(token) == expected_analysis

def test_stemmer_analyzer_lemmas(sample_tokens_stemmer: List[str]):
    """
    Tests the lemma extraction logic, which is a post-processing step.
    """
    stemmer = get_stemmer_analyzer()
    morph_results = stemmer.predict(sample_tokens_stemmer)
    
    # The lemma extraction is simple string splitting
    lemmas = [analysis.split('+')[0] for analysis in morph_results]

    expected_lemmas = {
        "Üniversite": "üniversite",
        "sınavlarına": "sınav",
        "çalışıyorlardı": "çalış",
        ".": ".",
    }

    result_dict = dict(zip(sample_tokens_stemmer, lemmas))
    for token, expected_lemma in expected_lemmas.items():
        assert result_dict.get(token) == expected_lemma

def test_stemmer_singleton_caching():
    """
    Tests that the get_stemmer_analyzer factory function reuses the instance.
    """
    import time
    
    # First initialization
    start_time_1 = time.time()
    stemmer1 = get_stemmer_analyzer()
    init_time_1 = time.time() - start_time_1
    
    # Second initialization should be near-instantaneous
    start_time_2 = time.time()
    stemmer2 = get_stemmer_analyzer()
    init_time_2 = time.time() - start_time_2
    
    assert stemmer1 is stemmer2
    assert init_time_2 &lt; 0.1
    assert init_time_2 &lt; init_time_1
  </file>

  <file path="./tests/test_ner.py">
# tests/test_ner.py
import pytest
from typing import List, Tuple

# Assuming the package is installed or in the python path
from vnlp_colab.ner.ner_colab import NamedEntityRecognizer
from vnlp_colab.utils_colab import setup_logging

# Initialize logging once for the test module
setup_logging()

@pytest.fixture(scope="module")
def sample_sentence_ner() -&gt; str:
    """Provides a sample sentence for NER tests."""
    return "Benim adım Melikşah ve VNGRS AI Takımı'nda çalışıyorum."

@pytest.fixture(scope="module")
def sample_tokens_ner() -&gt; List[str]:
    """Provides the corresponding tokens for the sample sentence."""
    return ["Benim", "adım", "Melikşah", "ve", "VNGRS", "AI", "Takımı'nda", "çalışıyorum", "."]

def test_spucontext_ner_predict(sample_sentence_ner: str, sample_tokens_ner: List[str]):
    """
    Unit test for the SPUContextNER model.
    Verifies output format, length, and key tag correctness.
    """
    ner = NamedEntityRecognizer(model='SPUContextNER')
    result = ner.predict(sentence=sample_sentence_ner, tokens=sample_tokens_ner)

    assert isinstance(result, list)
    assert len(result) == len(sample_tokens_ner)
    
    # Check that each item is a tuple of (string, string)
    assert all(isinstance(item, tuple) and len(item) == 2 for item in result)
    assert all(isinstance(item[0], str) and isinstance(item[1], str) for item in result)

    # Validate specific, high-confidence tags
    expected_tags = {
        "Melikşah": "PER",
        "VNGRS": "ORG",
        "AI": "ORG",
        "Takımı'nda": "ORG",
        "adım": "O",
    }

    result_dict = dict(result)
    for token, expected_tag in expected_tags.items():
        assert result_dict.get(token) == expected_tag

def test_charner_predict():
    """
    Unit test for the CharNER model.
    Note: CharNER does its own internal tokenization (WordPunct), so we test
    it with a raw sentence and check against its expected tokenization.
    """
    ner = NamedEntityRecognizer(model='CharNER')
    sentence = "VNGRS AI Takımı'nda çalışıyorum."
    result = ner.predict(sentence=sentence, tokens=[]) # Tokens are ignored by CharNER

    assert isinstance(result, list)
    
    result_dict = dict(result)
    
    # CharNER uses WordPunctTokenize, which splits "'nda"
    expected_tags = {
        "VNGRS": "ORG",
        "AI": "ORG",
        "Takımı": "ORG",
        "'": "ORG", # Continues the ORG entity
        "nda": "ORG",
        "çalışıyorum": "O",
    }
    
    for token, expected_tag in expected_tags.items():
        assert result_dict.get(token) == expected_tag

def test_ner_singleton_caching():
    """
    Tests that the NamedEntityRecognizer factory reuses instances.
    """
    import time
    
    # First initialization
    start_time_1 = time.time()
    ner1 = NamedEntityRecognizer(model='SPUContextNER')
    init_time_1 = time.time() - start_time_1
    
    # Second initialization should be near-instantaneous
    start_time_2 = time.time()
    ner2 = NamedEntityRecognizer(model='SPUContextNER')
    init_time_2 = time.time() - start_time_2
    
    assert ner1.instance is ner2.instance
    assert init_time_2 &lt; 0.1
    assert init_time_2 &lt; init_time_1
  </file>

  <file path="./tests/test_pipeline.py">
# tests/test_pipeline.py
import pytest
import pandas as pd
from pathlib import Path

# Assuming the package is installed or in the python path
from vnlp_colab.pipeline_colab import VNLPipeline

@pytest.fixture(scope="module")
def csv_fixture_path() -&gt; Path:
    """
    Creates a temporary dummy CSV file for pipeline testing.
    This fixture has a 'module' scope, so it's created once for all tests in this file.
    """
    dummy_data = (
        "novel01\t1\t1\t1\tOnun için yol arkadaşlarımızı titizlikle seçer, kendilerini iyice sınarız.\n"
        "novel01\t1\t1\t2\tBenim adım Melikşah ve İstanbul'da yaşıyorum.\n"
        "novel01\t1\t2\t1\tBu film harikaydı, çok beğendim.\n"
    )
    # In a Colab/testing environment, /tmp is a safe place for temporary files.
    csv_path = Path("/tmp/test_input_fixture.csv")
    csv_path.write_text(dummy_data, encoding='utf-8')
    
    yield csv_path
    
    # Teardown: clean up the file after tests are done
    csv_path.unlink()
    Path("/tmp/output.initial.pkl").unlink(missing_ok=True)
    Path("/tmp/output.pkl").unlink(missing_ok=True)
    Path("/tmp/treestack_output.pkl").unlink(missing_ok=True)
    Path("/tmp/treestack_output.initial.pkl").unlink(missing_ok=True)


def test_full_spucontext_pipeline(csv_fixture_path: Path):
    """
    Integration test for the VNLPipeline with all default SPUContext models.
    Verifies that the pipeline runs end-to-end and produces a DataFrame with the correct structure.
    """
    # 1. Initialize the pipeline with all core models
    models_to_run = ['pos', 'ner', 'dep', 'stemmer', 'sentiment']
    pipeline = VNLPipeline(models_to_load=models_to_run)
    output_path = "/tmp/output.pkl"

    # 2. Run the full pipeline
    final_df = pipeline.run(csv_path=str(csv_fixture_path), output_pickle_path=output_path)

    # 3. Validate the output DataFrame
    assert isinstance(final_df, pd.DataFrame)
    assert not final_df.empty
    assert len(final_df) == 3  # Check if all rows were processed

    expected_columns = [
        't_code', 'ch_no', 'p_no', 's_no', 'sentence', 'no_accents', 'tokens',
        'tokens_40', 'sentiment', 'morph', 'lemma', 'pos', 'ner', 'dep'
    ]
    for col in expected_columns:
        assert col in final_df.columns

    # Check that the analysis columns for the first row are populated and have the correct type
    first_row = final_df.iloc[0]
    assert isinstance(first_row['sentiment'], float)
    assert isinstance(first_row['morph'], list) and first_row['morph']
    assert isinstance(first_row['lemma'], list) and first_row['lemma']
    assert isinstance(first_row['pos'], list) and first_row['pos']
    assert isinstance(first_row['ner'], list) and first_row['ner']
    assert isinstance(first_row['dep'], list) and first_row['dep']
    assert len(first_row['tokens']) == len(first_row['pos']) == len(first_row['ner']) == len(first_row['dep'])

def test_treestack_dependency_pipeline(csv_fixture_path: Path):
    """
    Integration test for the pipeline with TreeStackDP.
    This implicitly tests the dependency resolution logic (stemmer -&gt; pos -&gt; dep).
    """
    # 1. Initialize with a model that has a dependency chain
    models_to_run = ['dep:TreeStackDP', 'sentiment'] # Should auto-load stemmer and pos:TreeStackPoS
    pipeline = VNLPipeline(models_to_load=models_to_run)
    output_path = "/tmp/treestack_output.pkl"
    
    # 2. Run the pipeline
    final_df = pipeline.run(csv_path=str(csv_fixture_path), output_pickle_path=output_path)

    # 3. Validate the output DataFrame
    assert isinstance(final_df, pd.DataFrame)
    assert not final_df.empty

    # Check that all dependencies were run and their columns exist
    expected_columns = ['morph', 'lemma', 'pos', 'dep', 'sentiment']
    for col in expected_columns:
        assert col in final_df.columns

    # Verify content of the first row
    first_row = final_df.iloc[0]
    assert isinstance(first_row['dep'], list) and first_row['dep']
    assert len(first_row['tokens']) == len(first_row['dep'])
  </file>

  <file path="./tests/test_pos.py">
# tests/test_pos.py
import pytest
from typing import List, Tuple

# Assuming the package is installed or in the python path
from vnlp_colab.pos.pos_colab import PoSTagger
from vnlp_colab.utils_colab import setup_logging

# Initialize logging once for the test module
setup_logging()

@pytest.fixture(scope="module")
def sample_tokens() -&gt; List[str]:
    """Provides a sample tokenized sentence for PoS tagging tests."""
    return ["Benim", "adım", "Melikşah", "ve", "İstanbul'da", "yaşıyorum", "."]

def test_spucontext_pos_tagger_predict(sample_tokens: List[str]):
    """
    Unit test for the SPUContextPoS model.
    Verifies output format, length, and key tag correctness.
    """
    pos_tagger = PoSTagger(model='SPUContextPoS')
    result = pos_tagger.predict(sample_tokens)

    assert isinstance(result, list)
    assert len(result) == len(sample_tokens)
    
    # Check that each item is a tuple of (string, string)
    assert all(isinstance(item, tuple) and len(item) == 2 for item in result)
    assert all(isinstance(item[0], str) and isinstance(item[1], str) for item in result)

    # Validate specific, high-confidence tags
    expected_tags = {
        "Melikşah": "PROPN",
        "ve": "CCONJ",
        "İstanbul'da": "PROPN",
        "yaşıyorum": "VERB",
        ".": "PUNCT",
    }

    result_dict = dict(result)
    for token, expected_tag in expected_tags.items():
        assert result_dict.get(token) == expected_tag

def test_treestack_pos_tagger_predict(sample_tokens: List[str]):
    """
    Unit test for the TreeStackPoS model.
    Verifies its end-to-end functionality, including its dependency on the stemmer.
    """
    pos_tagger = PoSTagger(model='TreeStackPoS')
    result = pos_tagger.predict(sample_tokens)

    assert isinstance(result, list)
    assert len(result) == len(sample_tokens)
    
    assert all(isinstance(item, tuple) and len(item) == 2 for item in result)

    # Validate specific, high-confidence tags
    expected_tags = {
        "Melikşah": "PROPN",
        "ve": "CCONJ",
        "İstanbul'da": "PROPN",
        "yaşıyorum": "VERB",
        ".": "PUNCT",
    }

    result_dict = dict(result)
    for token, expected_tag in expected_tags.items():
        assert result_dict.get(token) == expected_tag

def test_pos_tagger_singleton_caching():
    """
    Tests that the PoSTagger factory reuses instances instead of re-creating them.
    """
    import time
    
    # First initialization should be slow (downloads/loads model)
    start_time_1 = time.time()
    tagger1 = PoSTagger(model='SPUContextPoS')
    init_time_1 = time.time() - start_time_1
    
    # Second initialization should be near-instantaneous
    start_time_2 = time.time()
    tagger2 = PoSTagger(model='SPUContextPoS')
    init_time_2 = time.time() - start_time_2
    
    assert tagger1.instance is tagger2.instance
    assert init_time_2 &lt; 0.1  # Subsequent calls should be extremely fast
    assert init_time_2 &lt; init_time_1
  </file>

  <file path="./tests/test_dep.py">
# tests/test_dep.py
import pytest
from typing import List, Tuple

# Assuming the package is installed or in the python path
from vnlp_colab.dep.dep_colab import DependencyParser
from vnlp_colab.utils_colab import setup_logging

# Initialize logging once for the test module
setup_logging()

@pytest.fixture(scope="module")
def sample_tokens_dep() -&gt; List[str]:
    """Provides a sample tokenized sentence for Dependency Parsing tests."""
    return ["Onun", "için", "arkadaşlarımızı", "titizlikle", "seçeriz", "."]

def test_spucontext_dp_predict(sample_tokens_dep: List[str]):
    """
    Unit test for the SPUContextDP model.
    Verifies output format, length, and key dependency relations.
    """
    parser = DependencyParser(model='SPUContextDP')
    result = parser.predict(tokens=sample_tokens_dep)

    assert isinstance(result, list)
    assert len(result) == len(sample_tokens_dep)
    
    # Check that each item is a tuple of (int, str, int, str)
    assert all(isinstance(item, tuple) and len(item) == 4 for item in result)
    assert all(
        isinstance(item[0], int) and 
        isinstance(item[1], str) and 
        isinstance(item[2], int) and 
        isinstance(item[3], str) for item in result
    )

    # Validate specific, high-confidence dependency relations
    # Format: (token, (expected_head_index, expected_label))
    expected_relations = {
        "Onun": (2, "nmod"),        # Onun -&gt; için
        "için": (5, "obl"),         # için -&gt; seçeriz
        "arkadaşlarımızı": (5, "obj"), # arkadaşlarımızı -&gt; seçeriz
        "seçeriz": (0, "root"),       # seçeriz is the root
        ".": (5, "punct"),          # . -&gt; seçeriz
    }

    result_map = {item[1]: (item[2], item[3]) for item in result}
    for token, (expected_head, expected_label) in expected_relations.items():
        assert token in result_map
        assert result_map[token][0] == expected_head
        # We can be a bit flexible with labels if they are close synonyms
        assert result_map[token][1] in [expected_label, expected_label.lower()]

def test_treestack_dp_predict(sample_tokens_dep: List[str]):
    """
    Unit test for the TreeStackDP model.
    Verifies its end-to-end functionality, including dependencies.
    """
    parser = DependencyParser(model='TreeStackDP')
    result = parser.predict(tokens=sample_tokens_dep)

    assert isinstance(result, list)
    assert len(result) == len(sample_tokens_dep)
    assert all(isinstance(item, tuple) and len(item) == 4 for item in result)

    # Validate specific, high-confidence dependency relations
    expected_relations = {
        "için": (5, "obl"),
        "arkadaşlarımızı": (5, "obj"),
        "seçeriz": (0, "root"),
        ".": (5, "punct"),
    }

    result_map = {item[1]: (item[2], item[3]) for item in result}
    for token, (expected_head, expected_label) in expected_relations.items():
        assert token in result_map
        assert result_map[token][0] == expected_head
        assert result_map[token][1] in [expected_label, expected_label.lower()]

def test_dp_singleton_caching():
    """
    Tests that the DependencyParser factory reuses instances.
    """
    import time
    
    # First initialization
    start_time_1 = time.time()
    parser1 = DependencyParser(model='SPUContextDP')
    init_time_1 = time.time() - start_time_1
    
    # Second initialization should be near-instantaneous
    start_time_2 = time.time()
    parser2 = DependencyParser(model='SPUContextDP')
    init_time_2 = time.time() - start_time_2
    
    assert parser1.instance is parser2.instance
    assert init_time_2 &lt; 0.1
    assert init_time_2 &lt; init_time_1
  </file>
</files>
</AI-input-vnlp-colab-project_2025-11-14_11.26.zip.xml>
